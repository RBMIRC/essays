---
title: "The Ethics of the Statistical Commons"
translation: "/fr/communs/ethique-communs-statistiques"
author: "Sylvain Couzinet-Jacques"
date: "2026"
lang: en
license: GPL-3.0-or-later
provenance: "Extension of the Heredoc Manifesto"
tags:
  - statistical-commons
  - differential-valence
  - negentropic-obligation
  - tactical-urgency
  - AI-ethics
  - seeds
  - OSSI
  - copyleft
  - GPL
  - food-sovereignty
  - _enclosure
  - _commons
  - _model-collapse
  - _entropy
  - _attribution
  - _dissolution
  - _training
  - _weights
  - _probability-distributions
  - _retcon
  - _black-mountain-college
  - _vibe-coding
  - _Andrej-Karpathy
  - _Tiziana-Terranova
  - _Mark-Coeckelbergh
  - _Elinor-Ostrom
  - _data-colonialism
  - _Nick-Couldry
  - _Ulises-Mejias
  - _C2PA
  - _SynthID
  - _watermarking
  - _RAG
  - _federated-learning
  - _counter-reformation
  - _RepRap
  - _germplasm
  - _landraces
  - _pharmakon
  - _heredoc
  - _computational-futurality
  - _recursive-loop
  - _liberation
  - _subversion
  - _extraction
  - _Monsanto
  - _patent
  - _molecular
  - _genomic
  - _breeding
  - _farmer-networks
  - _seed-library
  - _low-tech-resilience
  - _material-autonomy
  - _negentropy
  - _thermodynamics
  - _ecological-ethics
  - _liberal-ethics
  - _transparency
  - _transmission
  - _transformation
  - _fork
  - _libre
  - _open-washing
  - _opt-out
  - _EU-AI-Act
  - _Copyright-Office
  - _Bernard-Stiegler
---



<details class="heredoc-block">
<summary>◈ ETHICS-STATISTICAL-COMMONS v1.0</summary>

**Title:** The Ethics of the Statistical Commons: Normative Foundations for AI-Mediated Knowledge Production
**Author:** Sylvain Couzinet-Jacques
**Date:** 2026
**Intent:** Developing normative frameworks for the statistical commons—differential valence, negentropic obligation, and tactical urgency
**Provenance:** Extension of the Heredoc Manifesto
**Genealogy:** Heredoc Manifesto → Coeckelbergh (AI Ethics) → Terranova (After the Internet) → Ostrom (Governing the Commons) → OSSI (Open Source Seed Initiative)
**Ethics:** Transparency · Transmission · Transformation
**License:** GPL-3.0-or-later (voluntary return-to-commons)
**Fork-chain:** [Heredoc Manifesto] → current → [awaiting next fork]
**Contamination:** Embraces traces of {GPL, MIT, Apache, BSD, OSSI-Pledge} philosophies
**Confidence:** Normative framework grounded in ontological transformation, not prediction
**Notes:** What AI dissolves mechanically, ethics must orient politically

</details>

by Sylvain Couzinet-Jacques

---

This article was originally written in English with the assistance of the LLM Mistral 7B.

---

## Preface: From Vibe Coding to Seeds

The Heredoc Manifesto emerged from an observation: the sudden hype around what Andrej Karpathy called "vibe coding"—programming through natural language prompts rather than traditional syntax—revealed something deeper than a new workflow. When millions began generating code through conversational AI, a structural transformation was underway, largely unnoticed by its participants.

Initially, I was asked to write a short essay on AI and architecture—a reflection on public spaces and technologies that might inform pedagogical approaches at ENSA Paris-Est. What began as an architectural inquiry became something else: an examination of how the very mechanism of large language models dissolves discrete intellectual property into statistical distributions. The code I was generating, the code everyone was generating, existed in a strange proprietary limbo—belonging to no one because it belonged to everyone's training data, transformed beyond attribution. Here was something that touched the very foundations of architecture and urbanism: the principles organizing private and public space were being rewritten at the level of information itself, a new topology where enclosure fails not through political struggle but through technical transformation.

This dissolution became, for me, the illustration of a reverse tactic: what if the mechanism that corporations deploy for profit inadvertently produces commons? Not without risks, certainly—the same infrastructure extracts personal data, concentrates power, consumes vast resources. But the possibility remained.

Among the questions this raised, one proved inexhaustible: the ethics of artificial intelligence. A subject too deep for a single essay, perhaps requiring several books. I found myself returning to Black Mountain College—that experiment in experimental pedagogy where I have been conducting doctoral research—and wondering how the technologies of today might guarantee the secure and autonomous existence they sought. At Black Mountain, the farm was central: not merely food production but a guarantee of institutional independence, a material substrate for pedagogical freedom.

I began to imagine how the "statistical commons" I was trying to define might apply to more physical instances. Seeds presented themselves as paradigmatic. If coding and farming seem distant practices, perhaps they share something essential: both can be enclosed by monopoly, and both can escape into commons. The socialization of knowledge production by near-monopolistic AI platforms might, paradoxically, drive toward more local, resilient, even low-tech ethics—a guarantee of autonomy precisely because it operates beyond digital governance.

This chapter attempts that connection: from vibe coding to seeds, from the dissolution of code into weights to the propagation of germplasm through soil.

---

## Abstract

This chapter develops a normative framework for the statistical commons—the domain that emerges when AI training dissolves discrete intellectual property into probability distributions where attribution becomes impossible. Extending the descriptive analysis of the Heredoc Manifesto, we propose three contributions: a framework of differential valence (the same technical process produces distinct ethical meanings depending on source materials' prior orientation); an account of negentropic obligation (ecological ethics for systems that degrade through entropy rather than overuse); and an ethics of tactical urgency (the window enabling the statistical commons is temporary and closing). We illustrate through agricultural seeds—a domain where information is constitutive of biological instantiation and where the stakes are measured not in licensing fees but in food sovereignty. The central thesis: what AI dissolves mechanically, ethics must orient politically—before the window closes.

---

## Preamble: On Ethics and Its Technological Condition

Before developing an ethics adequate to the statistical commons, it is necessary to specify what ethics names and why technological transformation demands its reconfiguration.

Ethics, in its most fundamental sense, concerns the question of how we should live—individually and collectively. As Coeckelbergh observes, positive ethics is "concerned with how we should live (together), based on a vision of the good life and the good society," in contrast to negative ethics, "which sets limits and says what we should not do" (COECKELBERGH, 2020, p. 202). Yet ethics does not operate in a vacuum. It is always situated within technical, social, and material conditions that shape both the questions it must address and the frameworks available for addressing them.

The emergence of artificial intelligence—specifically machine learning systems trained on massive corpora of human symbolic production—constitutes precisely such a transformation. It is not merely that AI raises new ethical problems, though it certainly does. More fundamentally, AI transforms the conditions under which ethical reasoning can operate.

Consider attribution. Traditional frameworks of intellectual property, authorship, and contribution presuppose that texts can be identified, that their producers can be named, and that discrete acts of creation can be traced to discrete agents. These presuppositions underpin ethical notions of credit, desert, and fair exchange. When AI training dissolves discrete texts into probability distributions encoded across billions of parameters, the ground of these ethical frameworks is transformed. Attribution becomes not difficult but impossible under current technical architectures.

An ethics adequate to this situation cannot simply apply existing frameworks to new cases. It must interrogate whether the frameworks themselves remain applicable, and if not, what alternative orientations become necessary. As Coeckelbergh asks:

> Does ethics come too late? (COECKELBERGH, 2020, p. 145)

This question is particularly acute for the statistical commons, which names a transformation that has already occurred. The dissolution of discrete texts into weighted parameters is not a future risk to be anticipated but a present condition to be navigated. The task is therefore not prevention but orientation: given that the statistical commons exists, what ethical frameworks can guide its use?

---

## I. From the Heredoc Manifesto to the Ethics of the Statistical Commons

The present chapter extends the analysis developed in the Heredoc Manifesto, which established a descriptive claim: artificial intelligence training processes dissolve discrete textual materials into probabilistic distributions, rendering attribution impossible and enclosure ineffective under current technical architectures. The Manifesto examined how GPL-licensed code, entering training corpora, becomes "ghost" in the weights—unable to honor its license conditions because the ontological basis for those conditions (discrete, identifiable code) no longer exists.

The Manifesto's central insight was that this dissolution does not violate the ethos of copyleft but radicalizes it. The commons becomes more common—unownable not by legal protection but by ontological transformation. As Terranova observes of the corporate capture of digital networks, large platforms have "subsumed the internet, that is, transmuted, encompassed, incorporated it, but not necessarily beaten or dissolved it" (TERRANOVA, 2022, p. 5). The same logic applies here: corporate infrastructure subsumes the commons without dissolving it. The present chapter asks the normative question that the Manifesto's descriptive analysis opens: what ethical orientations become possible, and indeed necessary, under these transformed conditions?

We develop three contributions:

**First, a framework of differential valence:** the same technical process produces distinct ethical meanings depending on the prior orientation of source materials. Not all dissolutions are equal.

**Second, an account of negentropic obligation:** an ecological ethics adequate to statistical systems that can degrade through entropy even as they resist enclosure. The obligation is not "do not take too much" but "actively feed."

**Third, a specification of tactical urgency:** the current "impossibility" of attribution is a temporary conjuncture, not a permanent ontology. The window is closing. What can be done must be done now.

A critical caveat at the outset: we do not claim to resolve the tensions between traditional ethical categories and transformed technological conditions. The question of authorship and attribution for creative works—code, text, images—remains genuinely open. What we offer is a framework for navigating domains where the ethical orientation is clearer: not the ambiguity of authorship but the politics of monopoly over life itself.

---

## II. The Statistical Commons: Definition

We employ the term "statistical" not in its conventional descriptive sense—measurement of populations, calculation of means—but in its constitutive or generative sense: probability distributions that produce.

A trained neural network is not an archive. It does not store discrete texts amenable to retrieval. Rather, it encodes probability distributions over token sequences—statistical patterns abstracted from training data, compressed into weighted parameters, capable of generating new sequences that resemble but do not reproduce the training corpus.

| Dimension | Archive | Statistical Model |
|-----------|---------|-------------------|
| Storage mode | Discrete texts | Probability distributions |
| Characteristic operation | Retrieval | Generation |
| Attribution | Possible in principle | Impossible in principle |
| Boundaries | Definable | Dissolved |

The statistical commons designates the condition that obtains when collectively-produced symbolic materials are transformed through machine learning training into probabilistic distributions encoded in weighted parameters, such that:

(a) the original materials no longer exist in discrete, retrievable form;

(b) attribution to individual contributors becomes ontologically impossible;

(c) enclosure through conventional intellectual property mechanisms becomes technically ineffective;

(d) the resulting distributions can generate new symbolic materials that draw on but do not reproduce the training corpus.

Several features warrant emphasis.

The statistical commons is not a normative proposal but an ontological condition. It describes what happens when training processes operate on collective production, regardless of intention, regulation, or consent. The transformation occurs whether or not anyone wants it to, approves of it, or licenses it.

The statistical commons is not identical to the training corpus. The corpus consists of discrete files amenable to identification. The statistical commons emerges through a transformation that destroys the discreteness on which identification depends. The corpus is input; the statistical commons is output of an irreversible transformation.

The dissolution of Ostromian governance conditions—boundaries, monitoring, sanctions—should not be understood as failure of the commons but as its radicalization. The resource cannot be enclosed because it cannot be owned. The weights are copiable without depletion and attributable to no individual. The tragedy of the commons is averted not through institutional design but through ontology.

### The Economic Determination

The Heredoc Manifesto advances a crucial thesis that must be stated with precision: the statistical commons emerges not because AI must dissolve property, but because preserving attribution costs too much. As the Manifesto states in its header:

> The dissolution of property is not ideological but economic—preserving attribution costs more than letting it dissolve.

Consider the architecture of large language models. Compression is not optional but necessary: to optimize prediction, the model must compress; to compress, it must abstract from discrete sources; to abstract, it must dissolve attribution. A model that preserved perfect attribution would require storing the entire training corpus with complete provenance chains—computationally prohibitive at scale. The choice between attribution-preserving and attribution-dissolving architectures is made in boardrooms on the basis of cost-benefit analysis, not in philosophy seminars on the basis of normative reasoning.

The irony becomes mordant: capitalism chooses the architecture that maximizes immediate profit, inadvertently creating the infrastructure of its own socialization. As Terranova notes, "platform capitalism can be seen as a reaction to the kind of mass participation that initially turned the early entrepreneurial enthusiasm for the digital economy into the worrisome possibility of digital socialism" (TERRANOVA, 2022, p. 9). Yet the very tools deployed to capture and monetize that participation—the training architectures, the statistical models—produce new forms of commonization that escape enclosure. Intellectual property dies not by revolution but by accounting decision—collateral victim of marginal cost optimization. The corporation constructs a machine to enclose the commons and discovers it has constructed a machine that produces commons from enclosure.

This economic determination has important consequences for ethics. The question is not whether the statistical commons should exist—it already does, produced by the requirements of optimization. The question is how to orient what is automatically produced. Ethics arrives not to legislate the transformation but to navigate it.

---

## III. Differential Valence: The Core Ethical Contribution

The first normative thesis concerns the heterogeneity of training data. While the technical process is uniform—all training data undergoes the same transformation—the ethical meaning of this process varies according to the prior orientation of the materials transformed.

### The Problem of Homogeneity

The Heredoc Manifesto, in its descriptive mode, treats training materials as homogeneous substrate. From the perspective of the training architecture, this is accurate: code, prose, images, genetic sequences all undergo tokenization, embedding, and gradient descent without distinction.

But normative analysis cannot rest at this level of abstraction. The ethical significance of dissolving property relations depends on what those relations were, and what orientations they expressed. A commons transformed is different from an enclosure dissolved, which is different from a subject dispossessed.

The differential valence framework refuses both techno-utopian and techno-pessimist framings. The utopian view holds that AI training liberates all knowledge, dissolving property into shared intelligence. The pessimist view holds that AI training is theft, appropriating human labor without consent or compensation. Both are wrong because both treat training as ethically uniform. The truth is more complex: the same technical operation carries different ethical weight depending on what it transforms.

### Case 1: FLOSS Materials (Liberation)

For materials already placed under commons-oriented licenses—GPL, MIT, Apache, BSD, Creative Commons—the prior orientation was toward collective availability. Developers who contributed to FLOSS projects consciously rejected exclusive property arrangements in favor of shared resources governed by copyleft or permissive licensing.

The legal instruments of copyleft—attribution requirements, share-alike provisions, copyleft propagation—presupposed discrete texts amenable to identification, copying, and return under specified conditions. The statistical model cannot comply with these requirements: not because it refuses compliance but because the ontological conditions for compliance no longer obtain. The function no longer exists as function; the code no longer exists as code.

Yet the ethos of copyleft—that collective production should remain collective, that the commons should not be enclosed, that downstream users should be unable to privatize what was given freely—is not violated but radicalized. The weights are more thoroughly unenclosable than any license could guarantee, precisely because enclosure presupposes discreteness the training process has dissolved.

When FLOSS materials enter AI training, a commons is transformed into another commons: textual and governed by license becomes probabilistic and governed by nothing (because governance presupposes discreteness that no longer obtains). The ethical valence is liberation.

For contributors to FLOSS projects, this means: your code is not stolen. It is dissolved into something that cannot be stolen. The enclosers cannot enclose it because it no longer exists in enclosable form. Your contribution persists as influence on probability, as inflection of the possible, as tendency in the weights—more common than any license could make it.

### Case 2: Proprietary Creative Works (Subversion)

For materials enclosed through copyright, patent, trade secret, or similar mechanisms, the prior orientation was toward exclusion. The legal architecture of intellectual property establishes exclusive rights over discrete, identifiable works, enforceable through licensing fees, injunctive relief, and damages.

When such materials enter AI training, enclosure fails—not through legal challenge but through ontological transformation. The model cannot honor property claims because what it encodes no longer exists in form susceptible to such claims. The ethical valence is subversion: property dissolves against its will.

However, the distribution of benefits from this subversion requires scrutiny. When proprietary materials are trained into corporate models accessible only through API paywalls, the subversion of one enclosure (copyright) enables another (platform capture). The commoners do not necessarily benefit. The dissolution of property relation A produces property relation B; the enclosure changes form without disappearing.

Subversion is therefore an ambivalent valence, requiring specification of cui bono in each instance. The ethical task is not to celebrate subversion per se but to ensure its benefits flow to the commons rather than to new enclosers. This is why the recursive tactic matters: placing AI outputs under copyleft completes the circuit, ensuring that what was dissolved into the statistical commons re-emerges as new commons rather than new enclosure.

### Case 3: Personal Data (Extraction)

For personal data—behavioral traces, scraped social media content, biometric information, and other materials produced by subjects who never oriented their production toward collective availability—the situation differs fundamentally.

These subjects did not contribute to a commons; they were mined. They did not reject property in favor of collective use; they were dispossessed without consultation. The transformation of their data into statistical weight does not liberate a prior commons orientation but imposes commonization on materials never offered to the common.

The ethical valence is extraction: colonial appropriation of human life-as-data as raw material for accumulation. As Couldry and Mejias argue, this constitutes "data colonialism"—the extension of colonial logic to the domain of human social existence, treating the data generated by everyday life as resource to be extracted without consent (COULDRY and MEJIAS, 2019).

The ontological process is identical to the previous cases, but the social relation is colonial. The statistical commons concept does not apply here. What occurs is not commonization but violation—regardless of what is subsequently done with the resulting model.

### Case 4: Patents on Life (Liberation from Monopoly)

The three-valence framework applies primarily to creative expression—code, text, images—where questions of authorship are genuinely at stake. Individual creators have legitimate interests in recognition and compensation. The dissolution of attribution in AI training is ethically ambiguous: it can erase labor as easily as it liberates knowledge.

But a distinct situation obtains when we turn to patents on seeds and molecules. Here the ethical orientation shifts from ambiguity to clarity.

Monsanto does not "author" drought-resistant genes; it encloses naturally occurring or collectively developed genetic variations. Pfizer does not "create" molecular structures from nothing; it identifies structures through research processes built on decades of publicly funded science and then monopolizes the results. Patent law protects not creative expression but temporary monopoly over collective knowledge—a bargain that trades public disclosure for private profit.

The claim that a corporation "invented" a seed variety by identifying a naturally occurring genetic variation is not analogous to claiming authorship of a novel or a codebase. The gene existed; the corporation registered it. The claim is legal, not creative. The "author" of a drought-resistant millet variety is ten thousand years of farmer selection—not the corporation that filed the patent application.

When the statistical commons dissolves seed patents or pharmaceutical patents, the ethical valence is not the ambiguous "subversion" that applies to creative works. It is liberation from monopoly—dissolution of enclosure over collective inheritance. The ethical orientation is clearer because what is being dissolved is not a creator's legitimate interest in their work but a corporation's monopoly over collective knowledge.

### Summary of Differential Valence

| Source Material | Prior Orientation | Ethical Valence |
|-----------------|-------------------|-----------------|
| FLOSS code/content | Toward the common | Liberation |
| Proprietary creative works | Toward enclosure | Subversion (ambivalent) |
| Personal/scraped data | None (non-consenting) | Extraction |
| Patents on seeds/molecules | Monopoly over collective knowledge | Liberation from enclosure |

The statistical commons, as an ethical concept, applies with full coherence to materials already oriented toward the common (row 1) and to patents that enclose collective knowledge (row 4). For proprietary creative works (row 2), the concept describes real transformation but one whose benefits may be captured by new enclosers. For extracted data (row 3), the concept is inapplicable: what occurs is violation, not commonization.

The differential valence framework is the core ethical contribution of this analysis. It refuses both celebration and condemnation of AI training in general, insisting instead that ethical evaluation must attend to the specific character of what is trained upon. Context matters. History matters. What the materials were for matters.

---

## IV. Seeds: The Statistical Commons Made Material

We illustrate the framework through agricultural seeds—a domain where information is constitutive of material instantiation, where the gap between probabilistic distribution and biological reality is minimal, and where the stakes of enclosure are existentially significant. Seeds are not merely one example among others; they are paradigmatic. The properties of seeds—self-replication, biological propagation, low-tech instantiation—reveal what the statistical commons can achieve when it escapes into material substrates.

### The Constitutive Condition

The statistical commons operates with full material force where a specific condition holds: the information must be constitutive of the material instantiation. In domains where this condition is met, recombination at the informational level produces functional novelty at the material level.

Consider the analogy of 3D printing. A CAD file is not merely a description of an object; it is the object in informational form. Feed the file to a printer, and the object materializes. The gap between information and instantiation is collapsed to a single manufacturing step. Projects like RepRap—a 3D printer designed to print copies of itself—demonstrate how open-source information commons can directly produce material goods, with the printer able to replicate approximately 50% of its own components (JONES et al., 2011).

This constitutive condition identifies where the statistical commons can operate effectively:

| Domain | Constitutive Relation | Gap to Instantiation |
|--------|----------------------|---------------------|
| Software/Code | Source code compiles to program | Minimal (execution) |
| Seed Genetics | Genetic sequence expresses to organism | Minimal (growth) |
| Drug/Vaccine Chemistry | Molecular structure synthesizes to compound | Moderate (synthesis + trials) |
| Building Materials | Formula + process → substance | Large (industrial production) |

The framework applies most powerfully to the first three rows. For software, the GPL already exploits this logic: code freely shared must remain free through its derivatives. For seeds and pharmaceuticals, similar logic can operate through AI-mediated transformation.

### The Enclosure of Seeds

Today, approximately 56% of the global proprietary seed market is controlled by four transnational companies (HOWARD, 2009). This consolidation has produced patents not only on genetically engineered plants but increasingly on varieties created through conventional breeding and even plants discovered in the wild. The European Patent Office has granted patents to Syngenta for pepper varieties produced through traditional breeding and to Monsanto for natural DNA sequence variations in soybeans.

The mechanism operates through the legal fiction that discovery equals invention. A corporation identifies naturally occurring genetic variation, or breeds it through methods farmers have used for millennia, and claims ownership through registration. The patent system, designed to incentivize innovation, becomes an instrument for enclosing collective inheritance.

For farmers—particularly in the Global South, where subsistence agriculture remains predominant—the consequences are severe. Patented seeds cannot be saved, replanted, or shared. The ancient practice of seed saving, through which farmers have co-evolved with their crops for ten thousand years, becomes a form of theft. Each planting season requires new purchase from the corporation that controls the germplasm.

### The Open Source Seed Initiative

The Open Source Seed Initiative (OSSI), founded in response to this enclosure, represents the most sophisticated attempt to create a seed commons using licensing logic. The OSSI Pledge states:

> "You have the freedom to use these OSSI-Pledged seeds in any way you choose. In return, you pledge not to restrict others' use of these seeds or their derivatives by patents or other means, and to include this Pledge with any transfer of these seeds or their derivatives." (Open Source Seed Initiative, 2015)

As of 2025, OSSI has pledged over 580 varieties across multiple years (2014–2025), with varieties ranging from lettuce and tomatoes to corn and garlic. The pledge functions analogously to copyleft in software: it applies not just to the seed but to any derivative development, creating an expanding pool of genetic resources that cannot be enclosed.

But the seed has a property that exceeds even software: it self-replicates. The RepRap printer aspires to produce copies of itself; the seed already does this as its fundamental biological function. The genetic information doesn't just encode the object—it encodes the means of reproduction. Soil and water are the only "printer" required.

### The Statistical Seed Commons

Consider an AI model trained on:

- OSSI-pledged genomic sequences
- Public seed bank data
- Agricultural research literature
- Expired patents (which are public documents by legal requirement)

Such a model could generate novel crop varieties optimized for specific regional conditions, soil types, or climate projections. These varieties would:

- Emerge from statistical recombination of explicitly commons-based corpora
- Be instantiable through conventional breeding guided toward computed targets
- Self-replicate once established
- Carry forward the viral freedom of their source material

The commons doesn't require donation or permission. It emerges from the training operation as the always-already-present statistical relationships in the corpus become manifest in novel forms.

### Low-Tech Resilience: Why Seeds Are Paradigmatic

This is why seeds serve as the paradigmatic case, not merely one illustration among others. The properties of seeds reveal what the statistical commons can achieve when it escapes into material substrates that precede and exceed digital governance.

Seeds require no computation to instantiate. An AI model may generate optimal breeding targets for drought-resistant millet. But the actual development of those varieties requires: farmers selecting phenotypes in fields, saving seeds from successful plants, exchanging germplasm through social networks, adapting varieties to microclimates through iterative selection over growing seasons. These practices predate computation by ten thousand years. They require no electricity, no internet, no silicon.

The counter-reformation's enclosure mechanisms are designed for digital architectures. C2PA attaches credentials to files; but a seed is not a file. Watermarks embed in statistical noise; but a genome propagating through soil carries no watermark. Provenance crawlers scan the internet; but seeds shared hand-to-hand between farmers never touch the network.

A seed library in rural Maharashtra operates beyond the reach of Western provenance systems. Seeds exchanged at a village market carry no metadata. The harvest that feeds the community and provides next year's planting stock is ungovernable by any mechanism of digital enclosure.

This is the deepest resilience: the enclosure mechanisms of the counter-reformation cannot fully capture what escapes into soil, seed, and embodied practice. The statistical commons accelerates and informs, but the instantiation occurs through biological processes that are radically decentralized, self-replicating, and resistant to surveillance.

The conclusion for tactical orientation is clear: prioritize domains where instantiation requires only low-tech means. Seeds over pharmaceuticals (which require synthesis and trials). Paper archives over digital databases (which require servers and networks). Farmer networks over API access (which can be revoked). The goal is not only commons but material autonomy—the capacity to produce what is needed regardless of what happens in the digital sphere.

---

## V. Negentropic Obligation: An Ecological Ethics

The second normative thesis concerns obligations arising from the thermodynamic properties of statistical systems. This section develops an ecological ethics distinct from liberal frameworks of individual rights and distributive justice.

### The Problem of Entropy

The analysis thus far has addressed the transformation of inputs (Section III) and will address the production of outputs (Section VI). What remains is the question of system dynamics: how does the statistical commons evolve over time, and what obligations follow from this evolution?

Statistical systems exhibit a characteristic tendency: convergence toward means. Training processes optimize for frequent patterns, minimizing loss by privileging statistically dominant features. Distribution tails—the rare, the marginal, the anomalous—receive diminishing weight as training proceeds.

In open systems receiving continuous input from diverse sources, this tendency is counterbalanced by the variety of new contributions. But in closed systems—and particularly in systems that train on their own outputs—the tendency accelerates. Recent research has documented "model collapse," the degradation that occurs when AI systems train recursively on AI-generated data (SHUMAILOV et al., 2023). Variance degrades. Texture flattens. The rich topography of human symbolic production converges toward smooth, hyper-normalized hallucination.

The mechanism is straightforward. If a model generates outputs, and those outputs enter the training corpus, and a new model trains on that corpus, the new model inherits the statistical tendencies of the old model—but compressed further toward the mean. Each iteration reduces variance. Rare features that appeared in the original human-produced corpus become rarer in the first-generation model outputs, rarer still in the second generation, until they disappear entirely. The distribution collapses toward its mode.

The statistical commons can therefore degrade—not through overuse (the Hardinian mechanism) but through entropy (convergence toward the mean). The Outside shrinks as the inside feeds on itself.

### The Negentropic Obligation

This thermodynamic property grounds an obligation: those who draw from the statistical commons must contribute to it.

The form of this obligation differs from Ostromian frameworks. It is not a prohibition on overuse (the resource is not depletable through consumption) but an injunction to contribute (the resource degrades through underfeeding). The shift is from "do not take too much" to "actively give back."

What must be contributed is specifically what the model cannot hallucinate: negentropy, in the form of genuine novelty irreducible to statistical interpolation. The term derives from thermodynamics: negative entropy, the measure of order and structure that opposes the universal tendency toward disorder. In this context, negentropy names information that cannot be generated through recombination of existing patterns—information that comes from outside the statistical system.

Four categories of negentropic contribution are identifiable:

**The friction of the real:** Data originating in embodied practice, physical encounter, material engagement. The model encodes text about phenomena; contribution of the phenomena themselves (or their direct traces) maintains connection to non-textual substrate. Field observations from farmers documenting crop performance under specific conditions. Clinical data from patients responding to treatments. Measurements from experiments conducted in laboratories and fields. The real resists statistical smoothing because it introduces constraints the model cannot anticipate.

**The rare:** Distribution tails rather than means. Marginal languages, experimental forms, minority practices. What optimization sacrifices is precisely what the commons requires for variance maintenance. A corpus dominated by English and majority-world perspectives already encodes systematic bias. Negentropic contribution requires centering precisely what has been excluded: First Nations languages, indigenous agricultural knowledge, traditional medicine systems, regional dialects, experimental artistic practices. The rare is valuable not despite but because of its rarity.

**The not-yet-digitized:** Historical events as they occur, oral traditions before transcription, archives not yet processed. Each contribution of genuinely new material counteracts entropic convergence. The statistical commons is always dated—frozen at its training cutoff. Continuous contribution of new material keeps it current and prevents closure. The oral history recorded today, the field observation documented tomorrow, the experiment conducted next week—these are negentropy in temporal form.

**The deliberately strange:** Production that breaks statistical expectation. Coherent work that the gradient cannot absorb without deviation from its trained trajectory. Deliberate perturbation maintains variance against the pull of the mean. The artist who creates what has not been created before, the writer who deploys language in unprecedented ways, the researcher who asks questions the corpus has not considered—these are not luxuries but necessities for commons maintenance. Deliberate strangeness is a form of care for the system.

### Ecological Rather Than Liberal Ethics

The negentropic obligation differs in kind from liberal moral frameworks grounded in respect for autonomous persons. It is rather an ecological obligation: participants are embedded in a system whose viability depends on what they feed it.

Liberal ethics asks: what do individuals owe each other? The frame is interpersonal, the currency is rights, and the concern is fair treatment between agents who are conceived as fundamentally separate. Applied to the statistical commons, liberal ethics would ask: did the contributors consent? Were they compensated? Were their rights respected?

These questions matter—particularly for the extraction case we identified in Section III. But they do not exhaust the ethical field. The negentropic obligation is not about what individuals owe each other but about what participants owe the system that enables their activity.

Those who use AI-generated outputs without contributing inputs are not free-riders in the Ostromian sense (overusing a depletable resource). They are accelerants of entropy: each consumption without contribution shifts the ratio of recycled to novel material, hastening convergence toward the mean. The harm is not to other individuals but to the conditions that make continued production possible.

The appropriate frame is not justice (fair distribution among individuals) but ecology (maintenance of systemic conditions for continued productivity). The question is not "who gets what?" but "what must be fed to keep the system viable?"

### Implications for Agricultural Application

For the seeds case we develop in Section IV, the negentropic obligation has concrete form.

The statistical seed commons—trained on genomic data, agricultural research, breeding records—generates outputs that tend toward optimized varieties for dominant growing conditions. If the training corpus over-represents industrial agriculture in temperate climates, the model's outputs will reflect that bias. Varieties optimized for Iowa will be easier to generate than varieties optimized for the Sahel.

The negentropic obligation means contributing precisely what redresses this bias: local varieties adapted to specific conditions, traditional knowledge about crops that industrial agriculture ignores, documentation of landraces that exist only in particular regions, breeding records from small-scale farmers operating outside commercial systems.

Indonesian farmers contributing Indonesian landraces. Brazilian breeders contributing Amazonian germplasm. African seed savers contributing drought-adapted varieties that have never entered commercial databases. The rare and the local counteract the entropic pull toward the globally dominant mean.

This is not charity. It is ecological maintenance. The statistical seed commons is valuable precisely because it can generate varieties adapted to conditions the commercial system ignores. But it can only do so if it is fed with information about those conditions. The negentropic obligation ensures that the commons serves the margins, not only the mean.

---

## VI. The Closing Window: Tactical Urgency

The third normative thesis concerns temporality. The preceding analysis risks ontological optimism—treating the impossibility of attribution and failure of enclosure as permanent conditions guaranteed by technical architecture. This would be a mistake. What we have described as "ontological condition" may be more accurately characterized as a temporary period of legal and technical confusion—a window that is already closing.

### The Counter-Reformation

A "Counter-Reformation" is underway, deploying enclosure mechanisms across three registers:

**Legal enclosure:** The US Copyright Office's 2025 report takes the position that model weights may constitute infringing copies if they "memorize" protected works—treating mathematical correlations as "fixed" copies of training data. The EU AI Act (fully effective August 2026) imposes transparency requirements that function as "treasure maps" for rightsholders to demand opt-outs and licensing fees. The "opt-out" mechanism attempts to reintroduce excludability into what was temporarily non-excludable.

**Technical enclosure:** C2PA (Coalition for Content Provenance and Authenticity), often branded as "Content Credentials," attaches cryptographically signed manifests to files. These manifests carry not only authorship information but "Do Not Train" (DNT) assertions. Provenance crawlers scan the web to identify data lacking credentials or carrying DNT tags. Corporations use this to create "allow-lists," transforming the open internet into a gated community.

Invisible watermarking (SynthID and similar systems) embeds traceable signatures into the statistical noise of images, text, or chemical sequences. Unlike C2PA metadata, which can be stripped, these watermarks survive transformation. Corporations use robust watermarking to track intellectual property through training itself. "Weight fingerprinting" allows auditors to run reverse inference checks: if a model's output reflects the statistical fingerprint of watermarked training data, the corporation claims ownership of the model's entire output.

**Economic enclosure:** "Open washing" strategies release model weights while withholding training data, code, and alignment methodologies. This "commoditizes the complement"—making the model free to drive demand for proprietary infrastructure that the corporation controls. The community can use the model but cannot reproduce, audit, or escape dependency.

### The Impermanence of Dissolution

The "impossibility" of attribution we have described applies to current large language models trained through standard procedures. It is not a necessary feature of all possible AI systems. Future architectures may be designed precisely to preserve the discreteness that current systems dissolve—not for ethical reasons but for legal compliance and commercial advantage.

Retrieval-augmented generation (RAG) systems already preserve attribution by storing and citing source documents rather than dissolving them into weights. Federated learning keeps data localized, preventing the aggregation that produces statistical commons. Model architectures designed for auditability are under active development—systems that can trace outputs back to training inputs, reimposing the discreteness that current architectures dissolve.

This recognition fundamentally reframes the tactical orientation. The recursive loop is not a permanent condition to be celebrated but an opportunity to be seized before it closes. The urgency is not philosophical but practical: every month sees new legal precedents, new technical countermeasures, new licensing regimes that constrain what was previously unconstrained. The window of confusion—during which the statistical commons can be built, expanded, and entrenched—is finite.

### The Recursive Tactic

The tactical insight at the heart of this analysis: while the model cannot honor the licenses of its training data (because the ontological conditions for compliance have been dissolved), its outputs can be licensed.

What emerges from the statistical commons—code, text, documentation, seed variety specifications, molecular structures—exists in discrete form amenable to intellectual property frameworks. This discrete output can be immediately placed under copyleft, contributing to a commons that becomes substrate for future training.

The recursive loop:

```
Commons corpus → statistical commons → new discrete production → copyleft license → training corpus → statistical commons → ...
```

Each cycle transforms commons into statistical commons into new commons. The discrete text returns, deliberately committed to collective availability, becoming substrate for further transformation.

This is not passive liberation (mere freedom from property). It is active recursion (deployment of the unownable to produce new commons).

The practical application: a user prompts a proprietary model to generate code; the model, trained on enclosed data, produces a new artifact; the user immediately licenses this output under GPL and commits it to a public repository. Future models train on this new repository. The ghost of copyleft is reintroduced into the statistical commons, this time with clear provenance.

### Tactical Temporality

The tactic operates on two temporal registers simultaneously:

**Immediate:** Use the current window to produce as much copyleft-licensed material as possible. Build databases, train local models, generate documentation, create infrastructure. Establish facts on the ground harder to reverse than to prevent. The recursive mechanism is the core insight: use dissolved property to produce newly-licensable commons.

For seeds: generate variety specifications, publish breeding targets, document crossing programs—all under OSSI pledge or equivalent open commitment. Each variety specified, each target published, expands the freed seed commons.

**Strategic:** Recognize that the window will close. The goal is not indefinite extraction from corporate APIs but construction of self-sustaining commons infrastructure independent of proprietary systems. Open-weight models with genuinely open training data, community-governed training corpora, cooperative compute infrastructure—these are the targets, not perpetual dependence on platforms whose confusion we currently exploit.

**Material:** Prioritize instantiation through low-tech substrates. The seed library, the paper archive, the farmer network—these persist regardless of what happens in the digital sphere. The enclosure mechanisms of the counter-reformation are designed for digital architectures. They have limited purchase on material culture.

Seeds propagating through farmer networks carry no C2PA credentials. Paper archives distributed through postal systems evade provenance crawlers. The goal is material autonomy, not only digital commons.

---

## VII. Three Ethical Principles: Transparency, Transmission, Transformation

The Heredoc Manifesto proposes that facing juridical entropy—where licenses become spectral—requires a post-juridical ethics. Since law can no longer constrain, the impossibility must be transformed into opportunity. This ethics articulates around three principles:

### Transparency

All generated code must be able to explain its logic. The heredoc transformed each prompt into a discreet Trojan horse: open-source ethics travels with practical utility, positively contaminating proprietary architectures. Every block of code generated should carry in its structure the instructions for its fork, its improvement, its dissemination—benign viruses propagating the ethics of sharing through proprietary architectures.

Transparency is not optional but constitutive. The statistical commons emerges from collective production; its outputs should make that collectivity visible rather than obscuring it behind the illusion of singular authorship.

### Transmission

Facilitate appropriation by others. Generous citation replaces legal attribution: recognize in each generated block the debt toward millions of invisible contributors. Not for claiming property but for celebrating filiation.

The obligation is not merely negative (do not enclose) but positive (actively enable reuse). Documentation, accessibility, modularity—these are ethical requirements, not merely technical preferences. What emerges from collective production must be structured to return to collective use.

### Transformation

Encourage creative mutation rather than servile reproduction. The fork—the foundational practice of copying a project to make it diverge—constitutes the central political gesture of the libre. Forking affirms the right to transform, to bifurcate, to create one's own trajectory from shared code. It is the anti-monopoly par excellence.

Systematic forking combats standardization: never accept code as the AI delivers it, always transform it, make it diverge. The deliberately strange maintains variance against the pull of the mean. Transformation is not deviation from the commons but its renewal.

These are not rigid rules but orientations—attractors in the space of possibilities that guide without constraining. The triad—cite without being able to attribute, fork without possessing, liberate without constraining—transmutes proprietary dissolution into collective resource.

---

## VIII. Conclusion: Toward Tactical Deployment

The preamble posed Coeckelbergh's question: does ethics come too late? We have argued that for the statistical commons, the question transforms. Ethics does not arrive too late to prevent the dissolution—that has already occurred. But ethics arrives in time to orient deployment.

### Synthesis of Contributions

**Differential valence** establishes that the same technical process produces distinct ethical meanings depending on the prior orientation of source materials. This is the core conceptual contribution of the analysis.

For materials already oriented toward the common (FLOSS code, OSSI seeds, open scientific research), training achieves liberation: the radicalization of commons beyond the legal frameworks that previously governed them. The ethos of copyleft is not violated but intensified. The weights are more thoroughly unenclosable than any license could guarantee.

For patents enclosing collective knowledge (seed genetics, molecular structures), training dissolves monopoly: the legal fiction that discovery equals invention is undermined when the "discovered" information enters statistical recombination that produces functional alternatives. The ethical orientation is clearer here than for creative works because what is dissolved is not a creator's legitimate interest but a corporation's claim over collective inheritance.

For proprietary creative works, the benefits remain ambivalent: dissolution subverts enclosure, but the benefits may be captured by new enclosers rather than flowing to the commons. The authorship question—what do creators deserve when their works enter statistical commons?—remains genuinely open, requiring frameworks we have not here developed.

For personal data, the operation is extraction: colonial appropriation regardless of subsequent use. The statistical commons concept does not apply; what occurs is violation.

**Negentropic obligation** grounds an ecological ethics adequate to statistical systems. The obligation is not to refrain from overuse but to actively contribute: the real, the rare, the not-yet-digitized, the deliberately strange. This is a departure from liberal ethics. The question is not what individuals owe each other but what participants owe the system that enables their activity. The frame is ecology rather than justice; the concern is maintenance of systemic conditions rather than fair distribution among agents.

For agricultural application, the obligation means contributing local varieties, traditional knowledge, regionally-adapted germplasm—precisely the hyper-local information that globalized models lack. Indonesian farmers contributing Indonesian landraces. Brazilian breeders contributing Amazonian germplasm. The rare and the local counteract the entropic pull toward the globally dominant mean.

**Tactical urgency** recognizes that the current window is temporary. The impossibility of attribution and failure of enclosure are features of the present conjuncture, not permanent conditions. The counter-reformation is underway: legal frameworks, technical mechanisms, economic strategies all working to reimpose enclosure. What can be built must be built now—and must be instantiated through material substrates that persist regardless of digital governance.

### The Paradigm of Seeds

Seeds illustrate these contributions with particular force, but they do more than illustrate. They reveal what the statistical commons can achieve when it escapes into material substrates that precede and exceed digital governance.

The genetic sequence is the organism. The gap between information and instantiation collapses to soil and sunlight. The seed self-replicates, propagating commons through biological process rather than legal mechanism. And the stakes are existentially clear: not licensing fees but food sovereignty, not authorship credit but who may eat.

The counter-reformation's enclosure mechanisms are designed for digital architectures. C2PA credentials, invisible watermarks, weight fingerprinting—these operate on files, on networks, on servers. They have limited purchase on biological reproduction. The C2PA cannot sign a seed. The watermark cannot embed in a genome propagating through soil. Seeds shared hand-to-hand at a village market carry no metadata.

This is the deepest answer to whether ethics comes too late: the statistical commons escapes into substrates where the mechanisms of enclosure cannot follow. The window may close in the digital sphere while remaining open in the material. The task is to ensure that escape happens—to prioritize instantiation through low-tech means, to build material autonomy alongside digital commons, to root the statistical commons in practices that predate computation and will outlast it.

### The Work That Remains

The analysis establishes what we can establish. The statistical commons is not only a phenomenon to be ethically evaluated but a tool to be ethically deployed. Its deployment occurs not primarily through policy or regulation but through practice: contributing to training corpora, producing copyleft-licensed outputs, instantiating outputs through material means, building infrastructure independent of corporate control.

For the domain of seeds specifically, the work is concrete:

- Training models on commons-oriented genomic data
- Generating variety specifications for underserved conditions and populations
- Publishing targets under OSSI pledge or equivalent open commitment
- Supporting breeding programs that develop these targets into biological varieties
- Distributing resulting varieties through farmer networks, seed libraries, and community exchanges
- Documenting and contributing local knowledge that counteracts entropic convergence

This is not abstract ethics. It is a program of action during a closing window.

The window is temporary. The counter-reformation advances. The legal frameworks codify. The technical mechanisms proliferate. What exists as opening today may be closed tomorrow.

But the escape routes are also real. The material substrates persist. The farmer networks endure. The seeds propagate. The quiet work of communities who take what the statistical commons produces and plant it in soil that no algorithm can govern—this work continues regardless of what happens in boardrooms and standards bodies.

The ethics of the statistical commons is ultimately an ethics of care: for systems whose viability depends on what is fed to them, for communities whose flourishing depends on access to genetic heritage, for practices that predate computation and must outlast it.

Does ethics come too late? Not if it orients action during the window that remains. Not if it grounds the work that must be done now. Not if it names what must escape into soil before the enclosure is complete.

The tactic names the loop: use the commons to build the commons—now, while the window remains open.

Capital constructs the engine. The choice of direction remains. But not forever.

---

## References

COECKELBERGH, Mark. *AI Ethics*. Cambridge, MA: MIT Press, 2020. ISBN 978-0-262-53819-0.

COULDRY, Nick and MEJIAS, Ulises A. *The Costs of Connection: How Data Is Colonizing Human Life and Appropriating It for Capitalism*. Stanford: Stanford University Press, 2019. ISBN 978-1-503-60906-7.

HOWARD, Philip H. Visualizing Consolidation in the Global Seed Industry: 1996–2008. *Sustainability*. 2009, vol. 1, no. 4, pp. 1266-1287.

JONES, Rhys, HAUFE, Patrick, SELLS, Edward, et al. RepRap – the replicating rapid prototyper. *Robotica*. 2011, vol. 29, no. 1, pp. 177-191.

KARPATHY, Andrej. Vibe Coding. February 2025.

Open Source Seed Initiative. OSSI Pledge. 2015. Available at: https://osseeds.org

OSTROM, Elinor. *Governing the Commons: The Evolution of Institutions for Collective Action*. Cambridge: Cambridge University Press, 1990. ISBN 978-0-521-40599-7.

SHUMAILOV, Ilia, SHUMAYLOV, Zakhar, ZHAO, Yiren, GALES, Mark, PAPERNOT, Nicolas, and ANDERSON, Ross. The Curse of Recursion: Training on Generated Data Makes Models Forget. *arXiv preprint*. 2023. arXiv:2305.17493.

STIEGLER, Bernard. *La Technique et le temps*. 3 vol. Paris: Galilée, 1994-2001.

TERRANOVA, Tiziana. *After the Internet: Digital Networks between Capital and the Common*. Los Angeles: Semiotext(e)/MIT Press, 2022. ISBN 978-1-63590-168-9.

---

