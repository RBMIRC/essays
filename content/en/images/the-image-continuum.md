---
title: "The Image Continuum"
translation: "/fr/images/le-continuum-de-limage"
subtitle: "On the Ontological Transformation of the Photograph in the Age of Generative Systems"
author: "Sylvain Couzinet-Jacques"
date: "2025"
lang: en
license: GPL-3.0-or-later
provenance: "Institut pour la Photographie, Lille"
tags:
  - image-continuum
  - post-indexical
  - generative-systems
  - photography
  - index
  - continuity
  - continuum
  - AI
  - latent-space
  - Barthes
  - Stiegler
  - Simondon
  - Flusser
  - Benjamin
  - tertiary-retention
  - individuation
  - punctum
  - studium
  - indexicality
  - GAN
  - training-data
  - statistical-archive
  - cosmotechnics
  - imaginary-media
  - retroactive-continuity
  - Florence-Thompson
  - Migrant-Mother
  - extraction
  - mean-image
---

<details class="heredoc-block">
<summary>◈ IMAGE-CONTINUUM v1.0</summary>

**Title:** The Image Continuum  
**Subtitle:** On the Ontological Transformation of the Photograph in the Age of Generative Systems  
**Author:** Sylvain Couzinet-Jacques  
**Date:** 2024  
**Intent:** From analog photograph to AI generated image  
**Provenance:** Institut pour la Photographie, Lille  
**Genealogy:** Barthes (Camera Lucida) → Stiegler (Technics and Time) → Simondon (Individuation) → Flusser (Technical Images) → Hui (Cosmotechnics)  
**Ethics:** Acknowledgment · Speculation · Technodiversity  
**License:** GPL-3.0-or-later  
**Fork-chain:** [Philosophy of technics] → current → [awaiting next fork]  
**Contamination:** Infused with {indexical theory, media archaeology, process philosophy, imaginary media}  
**Confidence:** Theoretical synthesis oriented toward practice  
**Notes:** The singular does not disappear; it is transformed  

</details>

## On the Photograph in the Age of Generative Systems

by Sylvain Couzinet-Jacques

---

## I. The Question

What becomes of the photograph when it ceases to be a photograph?

The question is not rhetorical. It designates a precise technical and philosophical problem that has emerged over the past decade, though its conditions were prepared throughout the history of photographic technology.

A photograph is taken. It enters an archive. The archive is ingested by a machine learning system. The system learns statistical regularities across millions of similar photographs. It then generates new images—images that bear all the perceptual marks of photography but are photographs of nothing. They depict faces that never existed, places that were never built, moments that never occurred. They resemble traces but are not. They present themselves as memories but remember nothing.

What is the ontological status of these generated images? The question matters because our entire visual culture—our practices of evidence, memory, documentation, art—has been built on assumptions about what photographs are and what they do. If these assumptions no longer hold, we must rethink the nature of the image and its relation to reality, memory, and truth.

Boris Groys, drawing on Nietzsche's concept of *Umwertung aller Werte* (revaluation of all values), argued that "innovation does not consist in the emergence of something previously hidden, but in the fact that the value of something always already seen and known is revaluated." For Nietzsche, the philosophical task was not to discover new truths but to overturn inherited hierarchies of value—to show that what passed for noble was in fact ressentiment, that what passed for truth was in fact interpretation. Groys transposes this logic into the field of art: artistic novelty lies not in the invention of unprecedented objects but in the displacement of existing objects from one regime of value to another.

The Duchampian readymade is paradigmatic of this operation. *Fountain* (1917)—an industrial urinal turned upside down and signed "R. Mutt"—did not create a new object. Duchamp fabricated nothing; he extracted a banal artifact from the commercial circuit and deposited it in the exhibition space. What changes is not the thing itself—it is still the same urinal, with the same material properties—but its value, its cultural position, its regime of attention. The object passes from the domain of the profane (sanitary equipment, the utilitarian, the invisible through excess of familiarity) to the domain of the sacred (the work of art, the contemplable, the worthy of interpretation). The artistic gesture is not production but *transvaluation*: a change of frame that transforms meaning without transforming matter.

This essay proposes that the trajectory from photograph to generated image is not a rupture but a continuum of successive revaluations—each transformation changing what the image is by changing what it does. The photograph becomes document, archival item, training instance, statistical contribution. But there is a mutation at the heart of this continuum. Groys's revaluation is cultural: accomplished by artists and institutions, it involves judgment, selection, attribution of meaning. The revaluation accomplished by the training process is statistical: the model does not judge value but learns probability; it does not ask what the photograph means but how frequently its features appear. The continuum thus traces the displacement of cultural judgment by computational averaging—revaluation without a revaluator. This is the ontological mutation this essay seeks to understand. The generated image does not destroy the photograph; it constitutes a critical node in the photograph's continuous becoming—a point where latent possibilities become manifest and new trajectories open. It makes explicit what the photograph has always been: not the capture of reality but the production of reality effects by technical means. To understand this claim, we must retrace the full arc of the image's transformation: from index to document to archive to statistical distribution to synthetic output. Each stage involves an ontological mutation—a change in what the image is—and yet each stage is also continuous with what precedes it. The continuum is not a smooth evolution but a series of phase transitions, each preserving something of the prior stage while introducing irreversible transformations.

---

## II. The Philosophy of Technics: A Theoretical Orientation

Before tracing the image continuum, we must establish the theoretical framework within which it becomes intelligible. I attempted to define a definition of the continuum in a preamble to this text by mobilizing what seemed to me an illuminating text by Luciana Parisi; I will not necessarily return to this definition. The framework I wish to establish here emerges from a tradition of thought that might be called the philosophy of technics—a line of investigation running from André Leroi-Gourhan through Gilbert Simondon and Bernard Stiegler, with important contributions from Vilém Flusser and, more recently, Yuk Hui. What unites these thinkers is a fundamental reorientation: technology is not a tool external to the human but constitutive of the human being itself.

Leroi-Gourhan, a paleontologist and anthropologist, proposed that human evolution cannot be understood independently of technical evolution. The human hand and the human tool co-evolved; the development of stone tools and the development of the brain capable of conceiving them are inseparable processes. Leroi-Gourhan called this *exteriorization*: the progressive delegation of bodily functions to technical supports. The stick extends the arm; the wheel extends the leg; writing extends memory. But these are not mere additions to a preexisting human nature. They are constitutive of what the human becomes. There is no human prior to technics; the human is, from the origin, a technical being.

Bernard Stiegler radicalized this intuition by arguing that technics constitutes human temporality itself. Drawing on Husserlian phenomenology of time-consciousness, Stiegler distinguished three forms of memory or "retention." Primary retention is the immediate holding of the just-past within present consciousness—the way the first notes of a melody remain present while we hear subsequent notes. Secondary retention is memory proper—the recall of past experiences, which are re-presented to consciousness after a delay. Tertiary retention is technical memory—the inscription of experience in external supports: writing, images, recordings, databases.

Stiegler's crucial move is to argue that tertiary retention is not merely supplementary to primary and secondary retention but conditions them. "Tertiary retention always already precedes the constitution of primary and secondary retention," he writes in *Technics and Time*. A newborn enters a world already saturated with technical memory—language, images, institutions, artifacts—and this technical inheritance shapes how the child will perceive and remember. We do not first experience and then record; rather, the existence of recording technologies shapes what we experience and how. A culture with photography perceives and remembers differently than a culture without it.

Stiegler called this originary technicity *epiphylogenesis*: "the pursuit of life by means other than life." Biological evolution proceeds by genetic inheritance; human evolution proceeds also by technical inheritance—the transmission of knowledge, practices, and artifacts across generations. Photography is an epiphylogenetic organ: it extends human memory beyond the biological limits of the nervous system, allowing experiences to be preserved and transmitted independently of individual brains.

But technics is not simply beneficial. Stiegler developed a pharmacology of technical objects, drawing on the Greek *pharmakon*—a word meaning both remedy and poison. Every technical system has both therapeutic and toxic potentials. Writing, as Plato's Socrates argues in the *Phaedrus*, is a *pharmakon*: it aids memory by providing external storage, but it also weakens memory by rendering such storage unnecessary. Photography is similarly pharmacological: it preserves the past but also freezes it, substitutes for living memory, and can be manipulated to falsify what it purports to record.

N. Katherine Hayles extends this intuition into the computational domain. In *Unthought: The Power of the Cognitive Nonconscious* (2017), she argues that human cognition has always been entangled with technical systems, forming what she calls "cognitive assemblages"—collectivities composed of humans, nonhumans, and computational media in which cognition, agency, and intentionality are distributed among many actors (HAYLES, 2017, p. 116). The generative model is such an assemblage—cognition without consciousness, processing without experience.

Luciana Parisi goes further. In a series of articles, notably "Automated Thinking and the Limits of Reason" (2016) and "Critical Computation: Digital Automata and General Artificial Thinking" (2019), she argues that machine learning systems operate through what she calls, following Charles Sanders Peirce, "abduction"—not the deductive reasoning that draws conclusions from given premises, nor the induction that generalizes from observed examples, but a speculative engagement with error, contingency, and the unknown. Where induction presupposes knowable objects and fixed concepts, abduction consists in creating new explanatory hypotheses in the face of unknown phenomena (PARISI, 2019, p. 12). The generative model does not calculate in the classical sense; it abduces, constructing hypotheses from patterns it cannot explain, learning "from incomplete information" in a process where "the incompleteness of models is a motor for speculative algorithms" (PARISI, 2019, p. 11).

Gilbert Simondon, whose work profoundly influenced Stiegler, approached technics from a different angle: the question of individuation. Traditional metaphysics begins with individuals—with fully formed beings whose existence is taken for granted. Simondon reversed this priority: individuals are not given but become. They emerge through processes of individuation from a pre-individual field of potentialities.

This pre-individual field is metastable: it contains tensions and potentials that can be resolved through individuation. When individuation occurs, a structured individual emerges, but the pre-individual field is not exhausted—potentials remain for further individuations. Simondon called the mechanism of individuation *transduction*: "an operation by which an activity propagates incrementally within a domain, each region of constituted structure serving as a principle of constitution for the next region."

Technical objects, no less than biological organisms, *individuate*. A technology does not spring fully formed from an inventor's mind; it emerges through a process of *concretization* in which disparate elements become progressively integrated. The early automobile was an assemblage of distinct systems (engine, transmission, chassis) that functioned independently; the mature automobile is a concrete technical individual in which these systems have become mutually adapted, each shaped by its relations to the others.

Simondon also emphasized the *associated milieu* of technical objects: the environment that is internal to their functioning. The associated milieu of a fish is water; the associated milieu of a combustion engine is the fuel, air, and exhaust system it both requires and produces. For photography, the associated milieu includes not only the chemical bath and the darkroom but also the entire apparatus of exhibition, distribution, cultural valuation, and imaginative investment.

Vilém Flusser, writing from a different tradition, developed a vocabulary for understanding what he called the *apparatus*—a tool that produces symbols rather than material goods. The camera is paradigmatic: it does not transform matter (like a hammer or a lathe) but produces images. The apparatus inscribes a *program*—a finite set of possibilities that determines what can and cannot be produced. The photographer does not simply use the camera; they "play" within its program, realizing possibilities that the apparatus has predefined.

For Flusser, the operator of an apparatus is a *functionary*—one who executes functions predetermined by the technical system. This is not necessarily pejorative; Flusser recognized that skilled photographers can "play against" the apparatus, seeking unrealized possibilities within its program. But the general tendency is toward the absorption of human agency into technical function. The apparatus thinks; the functionary executes.

Finally, Yuk Hui has recently challenged the presumption of a universal technology. Different civilizations, Hui argues, have developed different *cosmotechnics*—different technical systems embedded in different understandings of the relation between cosmos, humanity, and making. Western technology, shaped by Cartesian dualism and the project of mastering nature, is one cosmotechnics among others. Chinese technology, organized around the relation between *dao* (the way) and *qi* (the vessel), represents a different cosmotechnics with different ontological implications.

Hui's framework suggests that the photographic apparatus is not a neutral technology but a product of specifically Western cosmotechnics. The camera embodies Cartesian assumptions: the separation of observer and observed, the mathematization of vision, the ideal of objective representation. The indexical mythology of photography—the belief that the camera shows us the world as it really is—is not a universal truth but a culturally specific fantasy.

This theoretical orientation—the philosophy of technics as developed by Leroi-Gourhan, Parisi, Hayles, Simondon, Stiegler, Flusser, and Hui—provides the framework for understanding the image continuum as I attempt to sketch it. Photography is not a neutral recording device but an epiphylogenetic organ, a pharmacological system, a technical individual with its own associated milieu, an apparatus with its own program, a product of a specific cosmotechnics. To trace its transformation is to trace the becoming of a technical object through successive phases of individuation.

---

## III. The Indexical Dispensation

Photography arrived in the nineteenth century as a solution to an ancient problem: how to fix the fugitive image. The *camera obscura* had long projected images of the world onto surfaces, but these images vanished when the light source moved. Photography's innovation was chemical: a surface that remembered the light that struck it. "From today, painting is dead," Paul Delaroche reportedly declared upon seeing the daguerreotype in 1839. Whether or not he said it, the sentiment captures something true: photography introduced a new kind of image, an image that seemed to bypass human mediation entirely.

André Bazin articulated the ontological stakes with precision as early as 1945. In "The Ontology of the Photographic Image," he argued that photography satisfies a fundamental psychological need: "the preservation of life by a representation of life." But unlike painting, which preserves by resemblance, photography preserves by trace. The photographic image is "formed automatically, without the creative intervention of man." The camera, Bazin wrote, produces an image "which is the object itself, freed from the conditions of time and space that govern it." This is a remarkable claim: the photograph is not a representation of the object but the object itself, liberated from its material constraints.

Bazin's rhetoric seems excessive, but it points toward something real. Photography bears a different relation to its referent than a painting or drawing. This difference was theorized most rigorously by Charles Sanders Peirce, the American pragmatist whose semiotics provided the conceptual vocabulary for understanding photography's distinctiveness. Peirce distinguished three types of signs: icons (which signify by resemblance), symbols (which signify by convention), and indices (which signify by physical causation). Photography is an index: it is caused by its referent. Light reflected from a face strikes a photosensitive surface and produces a chemical change. The resulting image is a trace of that face—as a footprint is a trace of a foot, a scar is a trace of a wound. The photograph does not merely resemble the face; it was produced by the face.

Roland Barthes, in *Camera Lucida*, gave this indexicality its most poignant formulation. Writing in 1980, shortly after his mother's death, Barthes searched among her photographs for one that would capture her essence. He found it in an image from her childhood—the "Winter Garden" photograph, which he describes but never reproduces. Looking at this photograph, Barthes was struck by what he called the *ça-a-été*—the "that-has-been." The photograph certifies that what it shows "has been": this person existed, stood before this lens, at this moment. "Every photograph is a certificate of presence," Barthes wrote. The photograph does not say "this is what she looked like" (that would be iconic) but "she was there" (indexical). This is why photographs of the dead are so moving: they testify to a presence that is now absence. The photograph is a "message without a code"—direct evidence of a past existence.

Barthes also distinguished two elements of photographic experience: the *studium* and the *punctum*. The *studium* is the cultural, linguistic, and political interpretation of a photograph—what it shows, what it means, how it functions within systems of representation. The *punctum* is something else entirely: a detail that "pricks" the viewer, that escapes cultural coding and strikes with unmediated force. The *punctum* is the eruption of the real within the image—the trace of a singular existence that cannot be assimilated to general categories. It is the heart of indexicality: not the photograph's meaning but its being, its testimony to an existence that exceeds any interpretation.

Consider a photograph that would become one of the most iconic images of the twentieth century. In March 1936, Dorothea Lange was returning from a month-long assignment for the FSA when she passed a sign indicating a camp of pea pickers in Nipomo, California. She almost did not stop—she had already driven thirty kilometers before turning back. In the camp, she found Florence Owens Thompson, a 32-year-old Cherokee woman from Oklahoma, sitting in a makeshift tent with her children. The pea crop had frozen; there was no work. Lange took six photographs in approximately ten minutes. She never asked the woman's name. The photograph that became "*Migrant Mother*" was taken in the final moments: Thompson gazes into the distance, two children turned away, burying their faces in her shoulders, her right hand raised to her chin in a gesture at once protective and exhausted. The composition is almost that of a pietà: the mother as axis, the weight of care made visible. The image captures, in Barthesian vocabulary, both the *studium* (documentary evidence of the Great Depression, the failure of agricultural capitalism) and the *punctum*—the specific quality of Thompson's gaze, neither confrontational nor averted but looking beyond the camera toward some middle distance where the future ought to be; the lines around her mouth that belong to this woman and no other.

This indexical dispensation governed the cultural function of photography for a century and a half. Photographs served as evidence in courts, documents in archives, records in scientific research. Family albums preserved the faces of ancestors. Newspapers printed photographs as proof of events. The entire apparatus of photojournalism, forensic photography, and documentary practice depended on the assumption that photographs were traces of what they depicted—that to see a photograph was, in some sense, to see the thing itself.

But from its origin, the indexical dispensation was incomplete. Photographs do not simply record; they frame, select, expose, develop, print. Every photograph involves decisions: where to stand, when to press the shutter, how long to expose, which lens to use, how to develop the negative, how to crop and print the positive. These decisions are invisible in the final image, which presents itself as if it were an unmediated window onto reality. This is what Roland Barthes, in *Mythologies*, called the mythological function: the process by which cultural constructions present themselves as natural facts. The mythology of photography is its apparent transparency—its seeming to show us the world directly, without interpretation.

The danger of this mythology was recognized early. As Allan Sekula observed in his influential essay "The Body and the Archive," "the photograph is imagined to have a primitive core of meaning devoid of all cultural determination." This imagination is false. Photography is always already cultural: framed by conventions, embedded in institutions, interpreted through codes. The police mugshot, the passport photo, the family portrait, the news image—each operates within distinct institutional regimes that determine how it is made, circulated, and understood. But the mythology persists because it serves powerful interests. Photography-as-evidence supports systems of surveillance, identification, and control. Photography-as-memory supports industries of nostalgia and commemoration. Photography-as-art supports markets of authenticity and originality. None of these systems could function if photography were understood as simply another kind of constructed image. The indexical dispensation is thus not simply a theoretical position but an ideology—a way of seeing that serves particular arrangements of power.

Yet indexical photography already carried within it the conditions of its own transformation. Walter Benjamin, writing in 1936—the same year Lange photographed Thompson—recognized that mechanical reproduction does not simply copy objects but breaks what he called aura: the transmissibility of an object's entire historical trajectory, the chain linking material persistence to the testimony borne by its passage through time. The reproduced image detaches from the spatiotemporal coordinates that anchored the original, entering a regime of circulation where presence is structurally absent.

Benjamin's crucial insight is that reproduction does not falsify authenticity but renders the criterion inapplicable. The question "Is this an authentic trace?" presupposes a relation between copy and original that mass reproduction makes structurally unavailable. What emerges from this suspension is a compensatory structure: the reproduced image presents itself as if it testified to a singular existence—but this "as if" is now constitutive rather than deceptive. The mythology of photographic transparency operates in the space opened by auratic withdrawal.

This sets the stage for understanding subsequent phases of the continuum. If mechanical reproduction suspends the criterion of authenticity, statistical reproduction—the transformation of photographs into training data—extends this suspension into a new domain. The generated image lacks even a determinate original from which it is absent. Yet it inherits the compensatory structure: it presents itself as if it were a trace, as if it testified to an existence. Benjamin stands at the threshold between auratic presence and post-auratic simulation, glimpsing the structure that would eventually produce the generated image.

---

## IV. The Document and Its Institution

When a photograph enters an institution, it becomes a document. This transformation is not merely contextual but ontological: the photograph changes what it is by entering new relations.

Consider an employee identification photograph. The photograph is taken according to strict conventions: neutral background, frontal pose, diffuse lighting, no smiling. These conventions produce a specific type of image—an image that strips away context, expression, and individuality to produce a face as type. The employee becomes an instance of the category "employee," identifiable and comparable to other instances.

The photograph is then attached to a file. It acquires metadata: name, employee number, hire date, department. This metadata does not simply describe the photograph; it constitutes it as a document. Without the metadata, the photograph is just a face; with the metadata, it is an identification, a verification, a record. The document is the photograph plus its institutional frame.

Michel Foucault analyzed how documents function within what he called "regimes of truth"—the systems by which societies produce and authorize knowledge. In *The Archaeology of Knowledge* and *Discipline and Punish*, Foucault traced how modern institutions (prisons, hospitals, schools, factories) produce individuals as objects of knowledge through practices of documentation and classification. The document does not simply record truth; it produces truth by inscribing individuals within administrative categories. The employee photograph produces the employee as a knowable and manageable entity. It participates in what Foucault called "disciplinary power": a power that operates through documentation, classification, and normalization rather than through direct coercion.

Sekula extended this analysis specifically to photography. The nineteenth-century archive, he showed, developed as an instrument of identification and control. Francis Galton's composite portraits, Alphonse Bertillon's anthropometric systems, and the vast police archives of mugshots all aimed to render the body legible, to transform the singular face into a classifiable type. Photography became what Sekula called a "technology of social reproduction"—a means of maintaining social hierarchies by fixing individuals within predetermined categories.

The document thus transforms the photograph's indexicality. The photograph still points toward a past moment—this face before this lens—but this pointing is now subordinated to an institutional function. The document does not say "she was there" (Barthes's *ça-a-été*) but "she is authorized," "she belongs," "she is known." The photograph's testimony is captured by the institution and put to work for administrative ends.

Lange's photograph of Thompson entered this regime immediately. She submitted it to the Resettlement Administration; it was published in the *San Francisco News* on March 10, 1936, with a caption identifying the subject only as "*Migrant Mother*... destitute in a pea picker's camp, because the crop froze." The photograph became a document—evidence deployed to support federal relief efforts. Within days, ten tons of food were shipped to the camp. But Thompson and her family had already left. They never received any of that food. The photograph had entered the regime of documentation, serving ends entirely disconnected from the woman it depicted: it had become an instance of the category "migrant," her singular face put to authorize administrative and political ends from which she would never benefit.

This capture is not complete. The photograph retains an excess—details that escape the institutional frame. The specific texture of skin, the fall of a shadow, the nearly imperceptible asymmetry of a face: these are recorded by the photograph but not by the document. They constitute what Barthes called the *punctum*: the detail that "pricks" the viewer, that escapes the cultural coding of the image. The *punctum* is the residue of indexicality within the document—the trace of singular existence that persists despite institutional capture.

But this residue is fragile. As the photograph passes from the office to the archive, from the active file to storage, the *punctum* fades. No one looks at the photograph for its singular details anymore; it exists only as a retrievable record, defined by its metadata rather than its content. The photograph becomes what one might call a dormant index—still testifying to a past existence, but only for a reader who may never come.

---

## V. The Archive and Its Transcendence

The archive is not simply a storage place. It is, as we have seen through Stiegler, a condition of possibility of memory itself—a form of tertiary retention that precedes and shapes primary and secondary retention.

When the employee photograph enters the archive, it becomes part of this transcendental structure. It is no longer an active document serving immediate administrative needs; it is a potential document, available for retrieval but not currently retrieved. Its meaning is deferred into an indefinite future. Someone may consult it someday—a genealogist, a historian, a legal investigator—or perhaps never. The photograph exists in what might be called suspended signification: meaningful in principle, awaiting an act of interpretation that may never occur.

The archive thus transforms temporality. The photograph originally testified to a past moment: this face, this light, this instant. In the archive, this temporal orientation becomes complicated. The photograph still points backward (to the moment of exposure) but also forward (to potential future retrievals). It becomes what Jacques Derrida, in *Archive Fever*, called a "spectral" object—neither fully present nor fully absent, haunting the archive with a past that awaits future activation.

Derrida's meditation on the archive emphasized its dual structure: the archive is both a repository of the past and a projection toward the future. The Greek *arkhē* means both "origin" and "command"—the archive preserves origins and commands futures. Every archive is oriented toward what Derrida called the *à-venir*: it preserves documents for readers who do not yet exist, for questions that have not yet been asked. The archive is thus always incomplete, always open to what will come.

"*Migrant Mother*" entered the Library of Congress. As a work produced for a federal agency, it became public domain. It was reproduced in textbooks, museums, on a postage stamp—one of the most reproduced photographs in history, the icon of the Great Depression, the face through which a nation remembers its crisis. Thompson was not identified until 1978, when a journalist tracked her down. She was bitter: "I wish she hadn't taken my picture. I can't get a penny out of it. She didn't ask my name. She said she wouldn't sell the pictures. She said she'd send me a copy. She never did." This is tertiary retention at its most paradoxical: Thompson's face became a condition of cultural memory—we "remember" the Depression through her face—but she herself was erased in the very act of preservation. The archive preserved the image while displacing the person. She became a spectral presence in Derrida's precise sense: everywhere visible, nowhere compensated, her singularity dissolved into iconicity.

Wolfgang Ernst, developing what he calls "media archaeology," insists that archives must be understood not hermeneutically (as repositories of meaning) but operationally (as technical systems of storage and retrieval). The archive is a machine for managing time—for preserving past states and making them available to future operations. The photograph in the archive is not simply an image with a history; it is a data object with an address, subject to the archive's technical protocols of storage, indexing, and access.

This operational perspective reveals something important: the archive does not preserve photographs intact. It transforms them through the very acts of preservation. Each migration—from paper to microfilm to digital file—involves losses and additions. Metadata accumulates; file formats change. What is preserved is not the photograph itself but a version of the photograph, shaped by the technical decisions of each archival system.

The digital archive intensifies this transformation. When the employee photograph is scanned and stored as a JPEG, it becomes a sequence of numbers—pixel values encoded according to a compression algorithm. The image is no longer a physical trace (chemical changes on a surface) but an abstract pattern (mathematical relations among numerical values). The indexical link to the original face is now mediated by multiple technical translations: light to electricity (in the scanner), electricity to numbers (in the analog-to-digital converter), numbers to compressed numbers (in the JPEG algorithm). Each translation is lossy; each involves decisions about what to preserve and what to discard.

And yet, when we view the JPEG on a screen, we see a face. The technical mediations are invisible. The image presents itself as if it were still a direct trace of the original face, still testifying to a past presence. This is the mythology of the digital archive: it conceals its own operations behind an appearance of continuity.

But this apparent permanence masks a deeper instability. As Wendy Chun argues, digital media "depends on a degeneration actively denied and repressed." The screen's refresh cycle, the dynamic flow of information, the constant regeneration of data from storage—all of these involve micro-losses, format translations, entropy. "Digital media, supposed to be more permanent and durable than other media (film, paper, etc.)," Chun writes, "depends on a degeneration actively denied and repressed." The image is "frozen for human eyes only"; behind the apparent stability, information flows and degenerates. Preservation requires perpetual transformation.

The photographs that will eventually enter the training set are already products of this process—degraded copies of degraded copies, their indexical link to the original face attenuated through countless regenerations.

Friedrich Kittler insisted that media are not neutral containers but technical systems whose capacities to store, transmit, and process determine what can be remembered. The digital archive stores photographs not as images but as addressable data—and this format determines what will be available for retrieval, for circulation, and ultimately for training.

---

## VI. The Imaginary Media Framework

Before proceeding to the next transformation, we must introduce a concept that illuminates what has been happening all along: *imaginary media*.

Eric Kluitenberg, developing the work of Siegfried Zielinski and the media archaeology tradition, argues that all media are partly real and partly imagined. Without the material apparatus—cameras, lenses, sensors, computers—media cannot function. But equally, without the collective fantasies projected onto these apparatuses—dreams of telepresence, transparency, immediacy—media would not have the cultural power they possess. The imaginary dimension is not secondary to the real but constitutive of it: we build technologies to realize our fantasies, and our fantasies are shaped by the technologies we have built.

Kluitenberg identifies three types of imaginary media. First, impossible machines: devices that cannot be built but whose meanings nonetheless impact the real world of media—the universal translator, the telepathy device, the total memory apparatus. These impossible machines shape desires and expectations that influence the design and reception of actual technologies. Second, mythologized actualities: real technologies onto which ideological meanings have been projected—photography as "window onto reality," the telephone as "annihilation of distance," the internet as "global village." These mythologies naturalize what are in fact contingent technical arrangements. Third, compensatory apparatuses: machines designed to make up for the failures of human communication—technologies that promise to overcome the limitations of language, embodiment, and mortality.

Photography, in this framework, is a mythologized actuality. It is a real technology—a system of lenses, sensors, and recording media—but it has been invested with mythological meaning: the belief that it shows us the world as it really is. This mythology is not simply false; it is productive. It shapes how photographs are made, circulated, and interpreted. It creates expectations that photographers strive to fulfill. It establishes the cultural authority of photography.

Zielinski's variantology or "deep time of media" adds a historical dimension. Media do not evolve in a linear progression toward ever-greater perfection. They branch, mutate, and die out like species in an ecosystem. Dead media—technologies that failed or were abandoned—are not simply failures; they are paths not taken, possibilities that remain latent. Media history is not a tree with a single trunk but a rhizome with multiple roots and branches, some flourishing, some dormant, some extinct.

This perspective allows us to see the image continuum not as a teleological progression toward a final form but as a series of nodes in a continuous process of variation and selection. Each phase of the continuum—photograph, document, archive, training data, generated image—is a node where certain possibilities are actualized and others foreclosed. The generated image is not the end of the continuum but a node where new trajectories become possible.

---

## VII. The Dissolution of the Singular

Now comes the decisive transformation. The archived photograph is *scraped* from a website, downloaded by a research team, and incorporated into a training dataset for a machine learning model. This process, often described in neutral technical terms as "data collection," is more accurately understood as extraction—the enclosure of what had been a digital commons into proprietary computational infrastructure. As Kate Crawford argues in *Atlas of AI*, faces are mined like resources, images accumulated alongside millions of others to produce value for private capital, without knowledge, consent, or compensation.

"*Migrant Mother*" is certainly in these training sets—via the Library of Congress digital archive, Wikipedia, educational databases, and countless reproductions. When a model learns what "documentary photography" or "poverty" or "motherhood" looks like, Thompson's face is among the gravitational influences shaping its parameters. This is a second extraction, mirroring the structure of the first: just as Lange took the photograph without asking Thompson's name or offering compensation, AI scrapers take the image without consent or payment. Just as Thompson never received any of the relief her image helped generate, her estate receives nothing from the models her face helps train. The logic of extraction is continuous across eight decades.

What happens to the photograph in this process?

At first, nothing visible. The JPEG file still exists. It can still be opened, viewed, printed. The face is still there. But the photograph has entered a new kind of relation—not with human viewers but with a computational system that will process it in ways no human eye can follow.

The training process begins. The model—let us say a generative adversarial network, or GAN—is presented with millions of photographs. It does not "see" these photographs as we do; it processes them as arrays of numerical values. To understand what happens next, we must understand how GANs work.

A GAN consists of two neural networks engaged in an adversarial game. The first network is the generator: it takes random noise as input and transforms it through many layers of computation into an image. The second network is the discriminator: it takes images as input and attempts to classify them as "real" (from the training set) or "fake" (produced by the generator). The two networks are trained together: the generator tries to produce images that fool the discriminator, while the discriminator tries to correctly identify fakes. Through this adversarial dynamic, the generator learns to produce increasingly convincing images.

Let us follow the flow of data through these networks in more detail.

The generator begins with a latent vector: a list of random numbers, typically drawn from a standard normal distribution. Think of it as a set of coordinates—like longitude and latitude, but with 512 dimensions instead of two. This vector might have 512 dimensions—512 numbers specifying a point in latent space. This point is then passed through a series of transposed convolutional layers, which progressively increase spatial resolution while decreasing the number of channels. In simpler terms: the network gradually "unfolds" the abstract point into an image, like developing a photograph from a compressed code. At each layer, the network applies learned filters that detect and combine patterns. Early layers produce coarse structures (the general shape of a face); later layers produce fine details (skin texture, individual hairs).

Mathematically, each layer computes a weighted sum of its inputs, passes the result through a nonlinear activation function, and outputs the result to the next layer. The weights—millions of numerical parameters—are what the network learns during training. They encode the patterns that allow the network to transform noise into faces. These weights are not programmed by humans; they emerge from exposure to data, much as a child learns to recognize faces not from explicit rules but from repeated encounter.

The discriminator works in the opposite direction. It takes an image as input and passes it through a series of convolutional layers that progressively decrease spatial resolution while increasing abstraction. Where the generator unfolds a point into an image, the discriminator compresses an image into a judgment. Early layers detect edges and textures; later layers detect higher-order patterns like the presence of eyes or the symmetry of faces. The final layer produces a single number: *the probability that the image is real rather than fake*.

Training proceeds through backpropagation. The term is technical, but the principle is simple: when the network makes an error, that error is traced backward through each computation to determine which weights were responsible, and those weights are adjusted slightly. When the discriminator misclassifies a fake as real, the error signal propagates backward through the discriminator, adjusting its weights to better detect fakes. When the discriminator correctly identifies a fake, the error signal propagates through both networks: the discriminator is rewarded, and the generator is adjusted to produce more convincing fakes. Through thousands or millions of such iterations, both networks improve.

The key to understanding what happens to training photographs lies in the loss function—the mathematical objective that training seeks to minimize. The discriminator's loss measures its classification accuracy. The generator's loss measures how well the discriminator can detect its outputs. But these losses are calculated over the statistical distribution of the training set, not over individual images. This is crucial: the model does not learn "this is image #47,832"; it learns "this is what images look like in general." Individual photographs contribute to this learning, but only as drops contribute to a river—shaping the flow without remaining identifiable within it.

As training progresses, something remarkable happens. Individual photographs lose their identity. What the model learns is not any particular face but the statistical regularities that characterize faces in general: the typical arrangement of features, the distribution of skin tones, the patterns of shadow and light that result from common lighting conditions. Each training photograph exerts a small influence on the model's weights—a gravitational pull toward its particular configuration—but this influence is averaged with millions of others.

When training is complete, individual photographs no longer exist as such within the model. They have been dissolved into the collective statistics encoded in the weights. The model does not store images; it stores patterns—the statistical regularities that characterize the training distribution. It has learned what faces look like "in general," without retaining any particular face.

This is a new kind of archival relation. The classical archive preserves singulars: this photograph, retrievable at this address. The neural network accomplishes what might be called statistical archiving: it preserves the distribution while dissolving the instances. The employee photograph cannot be extracted from the trained model; there is no address where it is stored. And yet the photograph has not simply disappeared. It has left traces—infinitesimal influences on the model's weights, which collectively determine what kinds of images the model can generate.

One might call this the dissolution of the singular. The photograph, originally a singular trace of a singular moment, has become a statistical contribution to a collective distribution. Its singularity has not been preserved but transcended—incorporated into a higher-order pattern that includes but is not reducible to any individual instance. This statistical dust, moreover, does not float in an ethereal cloud; it resides in a "Stack" of physical infrastructures—data centers consuming megawatts of electricity, GPU farms with significant carbon footprints, networks of fiber-optic cables spanning oceans—all supported by supply chains of mineral extraction and invisible labor.

In Simondian terms, the training set has become a pre-individual field—a metastable reservoir of potentialities from which new *individuals* can emerge. Latent space is this pre-individual field in geometric form: a high-dimensional manifold in which each point corresponds to a possible image. The training process has sculpted this space, concentrating probability density in regions that correspond to face-like images and leaving other regions sparse or empty.

---

## VIII. Generation and Adversarial Dynamics

The model is now asked to generate a new face. A random latent vector is sampled—a list of numbers drawn from a standard probability distribution. This vector specifies a point in latent space. The generator transforms this point through its layers, gradually building an image: first coarse structures (the general shape of a face), then finer details (features, textures), then pixel-level specifics.

The generated face appears on screen. It is strikingly realistic. It has pores and stubble and the slight asymmetries of real faces. It has reflections in the eyes that suggest a specific lighting environment. It has the subtle grain and color rendition of a particular type of camera. It looks, in all perceptible ways, like a photograph.

But it is not a photograph of anyone. The face it depicts never existed. No light from any face caused this image. The generated face is photorealistic without being photographic: it mimics the appearance of photography without the underlying causal structure.

This mimicry is not accidental. It is the explicit goal of the adversarial training process. The discriminator, trained on real photographs, has learned to detect the subtle statistical signatures that characterize photographic images: the specific noise distributions that result from sensor electronics, the color correlation patterns that result from lens optics, the texture microstructures that result from optical resolution limits. These are what might be called the *qualia* of photography—the perceptual markers that code an image as "photographic" rather than painted or drawn or rendered.

The generator, to fool the discriminator, must learn to replicate these signatures. It must produce images that bear the statistical fingerprint of photography even though no photographic process was involved. This is why generated faces are so convincingly photographic: the adversarial dynamic forces the generator to internalize the qualia of the medium.

We can be more precise about these *qualia*: depth of field and bokeh, lens distortion and chromatic aberration, sensor noise with its characteristic spatial distributions, the color science of particular films and sensors. All of these optical signatures are present in the training photographs, and so the model learns to reproduce them. The generated image exhibits photographic qualia not because it was photographed but because it was trained on photographs.

Yet there is a critical dimension of this photorealism that must not be overlooked. As Hito Steyerl has argued, AI does not simply replicate "photography"; it replicates the most statistically probable photography—it regresses toward what she calls the "mean image." The adversarial dynamic, by optimizing for indistinguishability from the training distribution, produces a normalization that eliminates precisely what Barthes called the *punctum*: the singular, the aberrant, the detail that pricks. The generated face is convincing because it is average—because it has learned to suppress the weird, the marginal, the unprecedented in favor of a consensual visual norm.

The training distribution is not a neutral sample of human visual experience; it is the sediment of specific histories—economic, political, ideological, religious. Which images were produced depended on who had cameras; which were preserved depended on what institutions valued; which were digitized depended on funding priorities; which were scraped depended on what was publicly accessible online. What Tiziana Terranova calls the "Corporate Platform Complex"—the planetary infrastructure meshing communication and computation—determines both what is visible to scraping and who profits from aggregation. The "mean image" is not the average of all possible photographs but the average of photographs that survived these filters—a visual norm shaped by colonial archives, commercial stock photography, surveillance infrastructure, and the particular biases of the platforms that made images available for collection. The model does not learn "what faces look like" but "what faces looked like to the cameras that had the power to produce and preserve images." The generated image inherits this history even as it erases its traces.

The generated image thus occupies a paradoxical position. It presents itself as an index—a trace of a face that was there—but there is no face and no there. It has all the marks of indexicality (the qualia of photography) without the indexical structure (causation by a referent). It is, one might say, a simulated index or a synthetic trace: an image that testifies to nothing while seeming to testify to something. Joanna Zylinska names this broader condition the "perception machine": an assemblage of technical infrastructure, human perception, and social relations in which the distinction between image capture and image creation has become structurally blurred. We do not merely inhabit an image-world but an automated imagination—a technical universe where image production increasingly proceeds without us.

---

## IX. Latent Space as Pre-Individual Field

To understand what the generated image is, we must understand the space from which it emerges: latent space.

The latent space of a generative model is a high-dimensional geometric manifold—a space with hundreds or thousands of dimensions. Each point in this space corresponds to a possible image. The generator is a function that maps points from latent space to images: given a list of coordinates, it produces a specific pixel configuration.

But not all points in latent space produce meaningful images. The training process has shaped the space, concentrating probability density in regions that correspond to face-like images. The model has learned a probability distribution over latent space—a function that assigns higher probability to points corresponding to plausible faces and lower probability to points corresponding to noise or nonsense.

When we sample a random latent vector and feed it to the generator, we are sampling from this learned distribution. The generated image is, in a precise sense, a sample from the distribution of possible faces as learned from the training set. It is one face among all the faces the model could have generated—an actualization of the model's potentialities.

This is where Simondon's concept of individuation becomes illuminating. Latent space is a pre-individual field: a metastable reservoir of potentialities that have not yet been actualized as individuals. Each point in the space represents a potential face—a face that could be generated but has not yet been generated. The process of generation is an individuation: the actualization of a potential from the pre-individual field, the emergence of a determinate individual from an indeterminate background.

But unlike natural individuation, this process is repeatable and reversible. The same latent point can be decoded again to produce the same image (or nearly the same, given floating-point precision). Different latent points can be interpolated to produce intermediate images—faces that morph smoothly between two endpoints. The space of potentialities remains available, ready for further individuations. The generative model makes individuation operational—subject to technical control, repeatable at will.

This is what Flusser anticipated as the "universe of technical images"—a condition in which images no longer point toward a world outside themselves but constitute an autonomous realm. Latent space is this universe in mathematical form. It is a space of pure potentiality, a reservoir of images that have never been photographed, a formalized imagination. It is also the geometric form of what Mark Fisher called the slow cancellation of the future: all the training images—from 1936 and 2016, from Nipomo and São Paulo—collapsed into a single manifold where there is no before and after, only coordinates.

---

## X. The Techno-Imaginary Synthesis

We can now synthesize the various theoretical frameworks mobilized for the image continuum.

From Stiegler, we take the concept of tertiary retention: the photograph is an epiphylogenetic organ, an exteriorized memory that conditions what we remember and how. The archive is a system of tertiary retentions that precedes and shapes individual memory. The generated image is a new form of tertiary retention—a form that synthesizes new "memories" from the statistical structure of accumulated retentions.

From Simondon, we take the concept of individuation: the photograph individuates from the pre-individual field of possible exposures; the generated image individuates from the pre-individual field of latent space. Both are actualizations of potentials, but the potentials are structured differently: the photograph's potentials are optical (what could be in front of the lens), while the generated image's potentials are statistical (what could be sampled from the distribution).

From Flusser, we take the concept of the apparatus and its program: the camera is an apparatus that inscribes a program of possible photographs; the generative model is an apparatus that inscribes a program of possible images. Latent space is the program—the finite (though vast) set of images that the model can produce. The operator who prompts the model is a functionary, selecting among possibilities that the apparatus has predefined.

From Kluitenberg, we take the concept of imaginary media: photography has always involved an imaginary dimension—the myth of transparency, the fantasy of unmediated access to reality. The generated image makes this imaginary dimension explicit by internalizing it into the apparatus. Latent space is a formalized imagination: the space of possible images encoded as mathematical structure.

From Hui, we take the concept of cosmotechnics: the photographic apparatus is not a neutral technology but a product of Western cosmotechnics, shaped by Cartesian assumptions about the relation between subject, object, and representation. The generative model intensifies these assumptions: it reduces the image to pure computation, completing the Cartesian project of mathematizing nature.

Together, these frameworks allow us to understand the generated image as a node in the image continuum—a point where multiple trajectories converge and where new trajectories become possible. The generated image is:

- A tertiary retention that synthesizes rather than records
- An individuation from a statistical pre-individual field
- An output of an apparatus whose program is latent space
- A realization of the photographic imaginary liberated from indexical constraint
- A product of Western cosmotechnics at its limit point

The generated image is not the end of the continuum but a node where the continuum turns. What comes next—what new forms of image-making and image-being become possible—remains to be seen.

---

## XI. Exosomatic Memory and Its Transformations

The philosophy of technics, from Leroi-Gourhan to Parisi, understands technology as an extension of the human organism. The tool extends the hand; the wheel extends the foot; the computer extends the brain. Memory technologies—writing, recording, photography—extend the capacity for retention beyond the biological limits of the nervous system.

Photography, in this framework, is an exosomatic memory organ. It remembers what the brain cannot: the precise configuration of light at a specific moment, preserved indefinitely in stable material form. The archive is a vast exosomatic memory system, holding the accumulated visual experience of a civilization. Through photographs, we remember faces we never saw, places we never visited, events that occurred before our birth. Photography extends memory across time and space, connecting us to pasts that would otherwise be inaccessible.

The generative model transforms this exosomatic function in a fundamental way. It does not simply store memories (like the archive) or retrieve memories (like the photograph pulled from a drawer). It produces new images from the statistical structure of accumulated memories. The model has learned what faces look like by processing millions of photographs; it can now generate faces that are consistent with this learning but were never photographed.

Is this still memory? In a sense, yes: the generated face is causally dependent on the training set, which is itself a vast repository of photographic memories. The model remembers, in a statistical sense, what it was trained on. But it is a strange kind of memory—memory without singularity, without the preservation of particular past moments. The model does not remember this face or that face; it remembers the distribution of faces, the statistical regularities that characterize faces in general. Its memory is generic rather than specific.

One might say that the generative model extends human memory beyond the constraint of the past. Photography extends memory by preserving past moments; the generative model extends memory by producing possible moments that never occurred. This is not a memory prosthesis but an imagination prosthesis—a device for generating possibilities rather than retrieving actualities.

But this framing is also incomplete. The generative model does not imagine in the way humans imagine. There is no experience of imagining, no phenomenology of possibility-generation. The model executes mathematical operations; it has no inner life. If it imagines, it does so without a subject who imagines—imagination as pure operation, detached from any mind that would undergo it.

This detachment marks a threshold in the history of exteriorization. Leroi-Gourhan traced the progressive delegation of bodily functions to technical supports; Stiegler traced the progressive delegation of cognitive functions. The generative model represents a further step: the delegation of imagination itself. The capacity to envision possibilities—to produce images of what might be—has been exteriorized into a technical system that operates without human experience.

This is no longer extension in the classical sense. The tool that extends the hand remains under the hand's control; the photograph that extends memory remains available for human retrieval. But the generative model operates autonomously. Once trained, it can generate images indefinitely, without human input, without human experience, without human meaning. It is not an extension of human imagination but an excision—the cutting away of the imaginative function into an autonomous technical operation.

---

## XII. The Image That Remembers Nothing

Return to Thompson. Her photograph became a document, then an icon, then a statistical contribution to a training set, then one of countless influences on the parameters of a generative model. Now the model is asked to generate "a documentary photograph of an anxious mother during the Great Depression."

What would it produce? Something that echoes Thompson—the composition, the gaze directed beyond the lens, the children turned away, the sepia tones that code "historical document." The model would have learned from "*Migrant Mother*" a visual grammar: the frontal maternal figure as axis, the weight of care made visible, the iconography of dignified suffering. This grammar has become a template—reproduced in countless photographs of maternal distress since 1936, becoming a genre unto itself. The model learns what made the photograph successful: its composition, its iconographic resonance, its cultural readability. It learns the *studium*.

What it cannot learn is what made the photograph singular: whatever it was in Thompson's specific gaze on that day, the particular exhaustion in her hand raised to her chin, the way her children turned away from her shoulders at that moment in that frozen pea field. The *punctum*—Barthes's term for the detail that pricks, that wounds, that cuts through cultural coding—is precisely what escapes statistical regularity. It is by definition a deviation: the detail that does not fit the pattern, the singularity that exceeds the category. Statistical learning produces the probable. The *punctum* is the improbable—the contingent, the accidental, the unrepeatable. Thompson's gaze was not probable; it simply was. It happened once, in ten minutes, and it cannot be regenerated because it was never generated in the first place—it was encountered.

The generated image would thus be structurally *punctum*-less. Not because the model failed, but because it succeeded—at learning the distribution. The *punctum* is what falls outside the distribution: the statistical aberration, the noise to be smoothed, the singularity to be averaged. The generated "*Migrant Mother*" would be iconic without being indexical—the image as pure cultural citation, referring not to a woman in a pea field but to the accumulated visual memory of "Depression-era maternal suffering" that Thompson's face, among others, helped constitute. It would inherit the *studium* (the genre, the composition, the iconography) while eliminating the *punctum* (the singularity that belonged to her alone).

Elena Esposito poses the question directly: "What happens to the new in a world of algorithmic predictions? How can art still experiment with unprecedented possibilities, if the range of the possible is structured in advance by algorithmic procedures?" Latent space is precisely such a pre-structuring. It defines, through the geometry of training, which images are possible—which regions of the manifold have probability density, which are sparse or empty. The model cannot generate what falls outside its distribution; the unprecedented, the singular, the *punctum*-bearing image is structurally excluded not by censorship but by probability. The range of the imaginable has been algorithmically bounded. What appears as infinite generative capacity—the millions of possible faces—is in fact a finite topology, shaped by the statistical regularities of the training set. The "new" that the model produces is new only in the sense of not-yet-sampled—a point in latent space not yet decoded—but never new in the sense of exceeding the distribution that defines what is decodable.

This is what Thompson's photograph has become in the age of generative systems. It is present as infinitesimal influence on probability distributions, absent as singular person. Her face has shaped what the model learned about "documentary photography," "poverty," "motherhood"—and yet the model cannot remember her. It remembers only the distribution she helped constitute. She is everywhere and nowhere, present as pattern, absent as person, remembered as statistical dust.

---

## XIII. Toward a Name

What is the generated image? We have approached this question from multiple angles—indexicality, archival theory, philosophy of technics, imaginary media, cosmotechnics—and each has illuminated something while leaving something in shadow. No existing category quite fits.

The generated image is not a photograph: it has no referent, no indexical structure, no *that-has-been*.

It is not a painting or drawing: it was not made by human gesture, does not express individual intention, cannot be attributed to an author.

It is not a render: it was not designed from a model, does not depict a specified scene, does not execute a plan.

It is not a composite: it was not assembled from identifiable sources, does not combine preexisting elements in traceable ways.

It belongs to a new ontological category—a category for which we do not yet have an adequate name. Several possibilities present themselves.

**Statistical image:** an image generated by sampling from a learned probability distribution. This is technically accurate but phenomenologically empty; it does not capture what we see when we look at the generated face.

**Latent image:** an image that has been decoded from a point in latent space. But "latent" suggests something hidden waiting to be revealed, whereas the generated image was never hidden—it was produced in the act of generation.

**Synthetic trace:** an image that has the appearance of a trace without the structure of a trace. This captures the paradox but sounds like a contradiction in terms.

**Archival phantasm:** an image that the archive dreams when it processes itself through adversarial dynamics. This captures the spectral quality but risks obscuring the technical precision of the process.

Perhaps the most adequate term is **post-indexical image**: an image that comes after the indexical dispensation, that presupposes the history of photography and inherits its visual conventions, but that has been liberated from the indexical constraint. The post-indexical image resembles a trace but is not a trace; it presents the form of memory without the substance; it simulates singular existence without referring to any singular.

The "post" here is not merely chronological (after photography) but ontological (beyond the logic of the index). The post-indexical image does not abolish indexicality but sublates it—takes it up and transforms it into something new. The indexical mythology persists: we still see the generated face as if it were a trace, we still respond to it with the visceral certainty of the *that-has-been*. But this mythology now operates in a vacuum; it refers to nothing; it testifies to an existence that never was.

---

## XIV. The Continuum Completed

We can now see the full arc of the image continuum.

The photograph is a singular trace of a singular moment, produced by physical causation. It testifies: *this was there*. The document is the photograph captured by an institutional function, its singularity subordinated to administrative ends. It authorizes: *this is verified*. The archival item is the document in suspended signification, awaiting a retrieval that may never come. It potentializes: *this might be remembered*. The training instance is the archival item dissolved into statistical contribution, its singularity sacrificed to the collective distribution. It influences: *this shaped the model*. The generated image is the sampling from that distribution, the individuation of a new singular from a pre-individual field. It appears: *this could have been*.

Each stage is continuous with what precedes—each presupposes and transforms its predecessor—and yet each involves an ontological mutation. The photograph that becomes a document is still a photograph, but it is now also something else. The document that enters the archive is still a document, but it is now also something else. And so on, through each phase transition.

The generated image is not the completion of the continuum but its current node: a point where latent possibilities of the photographic apparatus become manifest in new form. Photography has always involved imagination—the imaginative supplementation that made the photograph appear transparent, natural, veridical. The generated image makes this imagination explicit by internalizing it into the apparatus. Cornelius Castoriadis defined imagination as the capacity "that something other than what exists come to be, and come to be as new or as other." Latent space is this capacity in mathematical form—a formalized imagination, the space of possible images encoded as geometric structure. But there is a displacement: Castoriadis's imagination belongs to a subject who creates worlds; latent space generates without a subject. It is imagination become operational, imagination without an imaginer.

What the continuum reveals is that photography was never simply an index. It was always an index plus a mythology—a trace plus the belief that traces are transparent windows onto reality. The generated image removes the trace and preserves the mythology. It is the photographic imaginary liberated from the constraint of actual existence.

This is why the generated image feels both unprecedented and familiar. It is unprecedented because it has no referent, no original, no indexical ground. It is familiar because it inherits all the visual conventions—all the qualia—of photography. We know how to look at it; we just do not know what we are looking at.

---

## XV. What Remains

Thompson died in 1983. Her gravestone in Modesto, California, reads: "FLORENCE LEONA THOMPSON — *Migrant Mother* — A Legend of the Strength of American Motherhood." Even in death, she is captioned—identified not by her name alone but by the title of the photograph that displaced her.

What remains of her in the generated image? Not her face—that has been transformed into statistical contribution. Not her memory—the model does not remember her. What remains is something harder to name: a statistical residue, an infinitesimal influence on probabilities, a presence that cannot be retrieved or identified. She has shaped what the model learned about faces, about documentary photography, about maternal suffering—and yet the model cannot name her, cannot retrieve her, cannot distinguish her contribution from those of millions of others.

This is the condition of the image in the age of generative systems. The singular does not disappear; it is transformed. The past does not vanish; it is transcended. Memory does not end; it is generalized. What we lose is not the images but the singular relation between image and referent—the guarantee that this image points to this existence. What we gain is something stranger: a productive imagination that can generate infinitely many singulars without reference, infinitely many faces without persons, infinitely many memories without pasts.

Whether this is liberation or loss depends on what we think images are for. If images are for preserving singular existence—for holding onto the *that-has-been*—then generative systems represent a kind of forgetting, a dissolution of the past into statistical noise. If images are for exploring possibilities—for imagining what might be—then generative systems represent an enormous expansion of capacity, an imagination prosthesis of unprecedented power.

The truth is probably that images are for both, and more. The image continuum does not resolve into a single meaning but opens onto new questions: What do we want from images? What do we owe to the singulars transformed into our datasets? What forms of memory, imagination, and responsibility are adequate to the post-indexical condition?

Thompson in 1936 did not know she was standing at an early point in this continuum. She was simply sitting in a tent with her children, waiting for the pea crop to thaw, when a woman with a camera turned around and came back. The light that fell on her face was the same light that has fallen on faces for millennia. But something was being set in motion. Her face would become, successively: a portrait of maternal distress, a document authorizing federal relief, an icon of a nation's crisis, a public domain asset reproduced on postage stamps, and finally a training instance—one among millions of faces teaching a machine what faces look like. She was, without knowing it, a contributor to a visual infrastructure that would outlast her: from portrait to icon to statistical weight, her image was revalued at each node of the continuum, everywhere and nowhere, named and forgotten.

This is the image continuum: from her face to our screens, from trace to synthesis, from singular existence to statistical possibility. It is not a story of loss or gain but of transformation—the transformation of what images are and what they can do. The generated face that appears on our screen, echoing her gaze without remembering her, is a node in this ongoing process. It will not be the last.

---

## Coda: Retroactive Continuity

The essay has traced a transformation. From index to document to archive to training instance to generated image, the photograph has passed through successive phases, each revealing what was latent in the previous phase. The generated image does not destroy the photograph; it validates it—confirms that the photograph was never simply the capture of reality but the production of reality effects by technical means. The continuum makes explicit what the indexical mythology concealed.

But to end with description is to remain a functionary of the apparatus—to trace the program without playing against it. If the post-indexical condition is pharmacological, as Stiegler would insist, then it contains not only toxicity but therapeutic potential. The question is not whether we can reverse the transformation of Thompson into statistical contribution—we cannot, and perhaps should not—but whether we can inhabit the continuum otherwise.

I propose the term *retroactive continuity*—borrowed from narrative practice of revising established facts to open new trajectories—as a methodology for engaging the post-indexical image ethically, aesthetically, and technically. Retroactive continuity does not deny what has occurred; it works through the transformation to expose contingencies and open futures. It operates in three movements: recursive, derivative, and prospective.

### The Archaeology of Extraction and Speculation

The first movement is backward—a media archaeology of the Stack that has materialized the transformation. This means exposing the logic of extraction that converted Thompson's face from singular trace to statistical contribution: the scraping, the accumulation, the enclosure of the digital commons into proprietary infrastructure. It means naming the data centers, the GPU farms, the supply chains of mineral extraction and invisible labor that support latent space. The recursive movement treats the archive not as a neutral repository but as a site of unaccounted debt—populated by presences whose contributions have been extracted without consent or compensation.

Spectral citation becomes a practice: acknowledging, wherever possible, the singulars that shaped the distribution. Thompson's gravestone—"*Migrant Mother* — A Legend of the Strength of American Motherhood"—is a failed citation: it names the document, not the person. A recursive practice would insist on the name beneath the caption, the life beneath the icon, the singular beneath the statistical. This is not nostalgia for indexicality; it is an ethics of acknowledgment within the post-indexical condition.

The second movement is lateral—navigating latent space not as passive sampling but as speculative individuation. If latent space is a pre-individual field of potentialities (Simondon), then generation is individuation: the actualization of a possible from the metastable reservoir. But individuation need not be random. It can be deliberate, exploratory, resistant.

This is what Flusser meant by "playing against the apparatus"—seeking unrealized possibilities within the program, actualizing what the apparatus tends to suppress. The adversarial dynamic produces the mean image (Steyerl): the statistically probable, the culturally consensual, the *punctum*-less. Derivative speculation plays against this tendency. It seeks the improbable regions of latent space, the aberrations, the statistical margins that might produce new singularities. Not the recovery of Thompson's *punctum* (impossible) but the emergence of *puncta* not yet known—singularities that could not have existed before the transformation made them possible.

The mean image is what Jean-François Lyotard called the new that "works"—innovation conformed to "the metaphysics of capital, which is a technology of time." It satisfies the discriminator, it looks like a photograph, it circulates without friction. Derivative speculation, by contrast, seeks what Lyotard called the *now*: not the accumulation of novelty but the question "Is it happening?"—the irruption of an event that cannot be assimilated to the logic of innovation. The apparatus tends toward the new that works; latent space, precisely because it is vast and underexplored, may still harbor regions where the question mark has not yet stopped—where the generated image might exceed its own probability, might pose a question rather than deliver a product.

### The Bifurcation of the Imaginable

The third movement is forward—toward technodiversity and the creation of new cosmotechnics. Hui's critique of mono-technology suggests that the current trajectory—the homogenization of all visual cultures into a single latent space, trained on a single distribution (mostly Western), optimized for a single objective (mostly commercial)—is not inevitable. It is one cosmotechnics among possible others.

The prospective movement asks: what other generative systems might exist? Models trained on different visual traditions—*shanshui* painting, Islamic geometric patterns, Aboriginal dot painting—would produce different latent spaces, different distributions, different imaginaries. Models trained with different objectives—not photorealism but resonance, not plausibility but transformation—would generate differently. Models governed differently—collective ownership, transparent sourcing, commons-oriented rather than enclosure-oriented—would relate differently to the singulars whose traces they carry.

This is the forking of the imaginable: the deliberate bifurcation of the image continuum into multiple trajectories rather than a single statistical monoculture. It ensures that the continuum remains a node of becoming—open to futures not yet computed—rather than a closed loop converging on the mean.

Thompson did not consent to become a node in this continuum. But she is one nonetheless—her face distributed across the weights, her gaze contributing to distributions, her singularity transformed and recombined into possibilities she could never have imagined. To practice retroactive continuity is not to undo this transformation but to acknowledge it, to work through it, to orient it toward futures she was never consulted about.

The image continuum does not end with her. It does not end with us. The question is whether it remains a mechanism of extraction—transforming singulars into value for elsewhere—or becomes a site of collective imagination, where the statistical residue of the transformed becomes a field for new forms of agency, new forms of memory, new forms of the possible—sustained by what the apparatus cannot compute: the irreducibly singular, what Donna Haraway might call the capacity for "making-with" that no statistical process can exhaust.

---

## References

BARTHES, Roland. *Camera Lucida: Reflections on Photography* [1980]. Translated by Richard Howard. New York: Hill and Wang, 1981.

BARTHES, Roland. *Mythologies* [1957]. Translated by Annette Lavers. New York: Hill and Wang, 1972.

BAZIN, André. "The Ontology of the Photographic Image" [1945]. In: *What Is Cinema?*, Vol. 1. Translated by Hugh Gray. Berkeley: University of California Press, 1967, pp. 9–16.

BENJAMIN, Walter. "The Work of Art in the Age of Mechanical Reproduction" [1936]. In: *Illuminations*. Translated by Harry Zohn. New York: Schocken Books, 1969, pp. 217–251.

CASTORIADIS, Cornelius. *The Imaginary Institution of Society* [1975]. Translated by Kathleen Blamey. Cambridge, MA: MIT Press, 1987.

CHUN, Wendy Hui Kyong. "The Enduring Ephemeral, or The Future Is a Memory." In: HUHTAMO, Erkki and PARIKKA, Jussi (eds.). *Media Archaeology: Approaches, Applications, and Implications*. Berkeley: University of California Press, 2011, pp. 184–203.

CRAWFORD, Kate. *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence*. New Haven: Yale University Press, 2021.

DERRIDA, Jacques. *Archive Fever: A Freudian Impression* [1995]. Translated by Eric Prenowitz. Chicago: University of Chicago Press, 1996.

ERNST, Wolfgang. *Digital Memory and the Archive*. Edited by Jussi Parikka. Minneapolis: University of Minnesota Press, 2013.

ESPOSITO, Elena. "Predicting Innovation: Artistic Novelty and Digital Forecast." In: LIJSTER, Thijs (ed.). *The Future of the New: Artistic Innovation in Times of Social Acceleration*. Amsterdam: Valiz, 2018, pp. 120–123.

FISHER, Mark. *Ghosts of My Life: Writings on Depression, Hauntology and Lost Futures*. Winchester: Zero Books, 2014.

FLUSSER, Vilém. *Towards a Philosophy of Photography* [1983]. Translated by Anthony Mathews. London: Reaktion Books, 2000.

FLUSSER, Vilém. *Into the Universe of Technical Images* [1985]. Translated by Nancy Ann Roth. Minneapolis: University of Minnesota Press, 2011.

FOUCAULT, Michel. *The Archaeology of Knowledge* [1969]. Translated by A. M. Sheridan Smith. New York: Pantheon Books, 1972.

FOUCAULT, Michel. *Discipline and Punish: The Birth of the Prison* [1975]. Translated by Alan Sheridan. New York: Vintage Books, 1977.

GOODFELLOW, Ian, et al. "Generative Adversarial Networks." *Advances in Neural Information Processing Systems*. 2014, vol. 27, pp. 2672–2680.

GROYS, Boris. *On the New* [1992]. Translated by G. M. Goshgarian. London: Verso, 2014.

HARAWAY, Donna. *Staying with the Trouble: Making Kin in the Chthulucene*. Durham: Duke University Press, 2016.

HAYLES, N. Katherine. *Unthought: The Power of the Cognitive Nonconscious*. Chicago: University of Chicago Press, 2017.

HUI, Yuk. *The Question Concerning Technology in China: An Essay in Cosmotechnics*. Falmouth: Urbanomic, 2016.

HUI, Yuk. "Cosmotechnics as Cosmopolitics." *e-flux journal*. November 2017, no. 86.

HUI, Yuk. *Art and Cosmotechnics*. Minneapolis: University of Minnesota Press, 2021.

KITTLER, Friedrich. *Gramophone, Film, Typewriter* [1986]. Translated by Geoffrey Winthrop-Young and Michael Wutz. Stanford: Stanford University Press, 1999.

KLUITENBERG, Eric (ed.). *Book of Imaginary Media: Excavating the Dream of the Ultimate Communication Medium*. Amsterdam: NAi Publishers, 2006.

KLUITENBERG, Eric. "On the Archaeology of Imaginary Media." In: HUHTAMO, Erkki and PARIKKA, Jussi (eds.). *Media Archaeology: Approaches, Applications, and Implications*. Berkeley: University of California Press, 2011, pp. 48–69.

KRAUSS, Rosalind. "Notes on the Index: Seventies Art in America." *October*. Spring 1977, no. 3, pp. 68–81.

LEROI-GOURHAN, André. *Gesture and Speech* [1964–65]. Translated by Anna Bostock Berger. Cambridge, MA: MIT Press, 1993.

LYOTARD, Jean-François. "The Sublime and the Avant-Garde" [1988]. In: *The Inhuman: Reflections on Time*. Translated by Geoffrey Bennington and Rachel Bowlby. Stanford: Stanford University Press, 1991, pp. 89–107.

MITCHELL, William J. *The Reconfigured Eye: Visual Truth in the Post-Photographic Era*. Cambridge, MA: MIT Press, 1992.

PARISI, Luciana. "Critical Computation: Digital Automata and General Artificial Thinking." *Theory, Culture & Society*. 2019, vol. 36, no. 2, pp. 89–121.

PEIRCE, Charles Sanders. *Collected Papers of Charles Sanders Peirce*. Edited by Charles Hartshorne and Paul Weiss. Cambridge, MA: Harvard University Press, 1931–1958.

SEKULA, Allan. "The Body and the Archive." *October*. Winter 1986, no. 39, pp. 3–64.

SIMONDON, Gilbert. *On the Mode of Existence of Technical Objects* [1958]. Translated by Cécile Malaspina and John Rogove. Minneapolis: Univocal, 2017.

SIMONDON, Gilbert. *Individuation in Light of Notions of Form and Information* [2005]. Translated by Taylor Adkins. Minneapolis: University of Minnesota Press, 2020.

STEYERL, Hito. "Mean Images." *New Left Review*. March–June 2023, no. 140/141, pp. 82–97.

STIEGLER, Bernard. *Technics and Time, 1: The Fault of Epimetheus* [1994]. Translated by Richard Beardsworth and George Collins. Stanford: Stanford University Press, 1998.

STIEGLER, Bernard. *Technics and Time, 2: Disorientation* [1996]. Translated by Stephen Barker. Stanford: Stanford University Press, 2009.

STIEGLER, Bernard. *Technics and Time, 3: Cinematic Time and the Question of Malaise* [2001]. Translated by Stephen Barker. Stanford: Stanford University Press, 2011.

TERRANOVA, Tiziana. *After the Internet: Digital Networks between Capital and the Common*. South Pasadena: Semiotext(e), 2022.

ZIELINSKI, Siegfried. *Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means* [2002]. Translated by Gloria Custance. Cambridge, MA: MIT Press, 2006.

ZYLINSKA, Joanna. *The Perception Machine: Our Photographic Future between the Eye and AI*. Cambridge, MA: MIT Press, 2023.

---

Essay prepared in December 2024
Centre Norbert Elias (EHESS/Aix-Marseille)

---
