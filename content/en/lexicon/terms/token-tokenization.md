---
title: "Token / Tokenization"
type: glossary
aliases: ["token", "tokenization", "tokens"]
tags:
  - glossary
  - _heredoc-manifesto
---

The process of fragmenting text or code into elementary units ("tokens") that the algorithm manipulates mathematically. This is the first step in dissolving semantic integrityâ€”meaningful code becomes numerical vectors, preparing it for statistical compression. Tokenization begins the transformation from authored artifact to statistical pattern. (*The Heredoc Manifesto*)

**See also:**
- [[/en/lexicon/terms/embedding|Embedding]]
- [[/en/lexicon/terms/training|Training]]
- [[/en/lexicon/terms/latent-space|Latent Space]]
