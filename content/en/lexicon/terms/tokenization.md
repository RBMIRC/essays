---
title: "Tokenization"
type: glossary
aliases: []
tags:
  - glossary
  - _index
---

The process by which Large Language Models cut text into units (tokens) that correspond neither to words nor to concepts but to fragments statistically optimal for processing. "Understanding" becomes "under" + "standing"; unusual words become sequences of common fragments. The document loses its own syntax to enter a computational grammar. Part of the transductive passage between heterogeneous semiotic regimes. (*Index*)