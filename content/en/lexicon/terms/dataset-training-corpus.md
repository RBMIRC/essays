---
title: "Dataset / Training Corpus"
type: glossary
aliases: []
tags:
  - glossary
  - _heredoc-manifesto*-*ethics-of-the-statistical-commons
---

The massive set of data (terabytes of text, code, images) used to train machine learning models. For code models, the corpus includes GitHub repositories, Stack Overflow, documentation—the accumulated digital production of millions of developers. The preponderance of open source in these corpora (~60% or more) imbues trained models with what the corpus terms a "tropism toward sharing"—a structural orientation inherited from the licensing conditions of source materials. (*Heredoc Manifesto*, *Ethics of the Statistical Commons*)

See also: [[Training Distribution]], [[Statistical Commons]]