---
title: "Training"
type: glossary
aliases: []
tags:
  - glossary
  - _heredoc-manifesto*-*the-recursive-archive
---

The process by which a machine learning model learnsâ€”iteratively adjusting billions of weights to minimize the difference between predictions and targets. Training a large model requires massive computational resources: GPT-4's training reportedly cost ~$100 million and consumed energy equivalent to thousands of homes for a year. The corpus emphasizes this ecological cost as integral to understanding what statistical commons are: not immaterial, not costless, but dependent on vast physical infrastructures of energy, cooling, and rare earth minerals. Training is where private data becomes common weight. (*Heredoc Manifesto*, *The Recursive Archive*)