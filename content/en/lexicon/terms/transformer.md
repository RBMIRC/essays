---
title: "Transformer"
type: glossary
aliases: ["transformer architecture"]
tags:
  - glossary
  - _heredoc-manifesto
---

Neural network architecture (2017) that made modern LLMs possible through the attention mechanism. The transformer accelerates compression and recombination, enabling models to process vast contexts and generate coherent outputs. This architecture is the technical condition of possibility for the statistical commons described in the manifesto. (*The Heredoc Manifesto*)

**References:**
- VASWANI, Ashish et al. Attention Is All You Need. *Advances in Neural Information Processing Systems*. 2017, vol. 30.

**See also:**
- [[/en/lexicon/terms/llm-large-language-model|LLM]]
- [[/en/lexicon/terms/artificial-neural-network|Artificial Neural Network]]
- [[/en/lexicon/terms/deep-learning|Deep Learning]]
