---
title: "Transformer"
type: glossary
aliases: []
tags:
  - glossary
  - _heredoc-manifesto
---

A neural network architecture introduced in 2017 that made modern large language models possible. The key innovation is the "attention mechanism"â€”the ability to weigh the relevance of different parts of the input when producing each part of the output. Transformers process sequences in parallel (unlike earlier recurrent architectures), enabling training on much larger datasets. The architecture accelerated the compression and recombination that dissolves attribution: more data, more parameters, more thorough mixing. (*Heredoc Manifesto*)

**References:**
- VASWANI, Ashish et al. Attention Is All You Need. In: *Advances in Neural Information Processing Systems 30 (NIPS 2017)*. 2017.