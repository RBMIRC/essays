---
title: "Parameter / Weight"
type: glossary
aliases: []
tags:
  - glossary
  - _heredoc-manifesto
---

An adjustable floating-point number constituting the neural network's "memory"â€”the physical materialization of statistical commons. A large language model may have hundreds of billions of parameters; each encodes complex statistical correlations extracted from training data. No individual parameter corresponds to a specific piece of knowledge; understanding is distributed across the entire network. This distributed encoding is what makes attribution impossible: the "knowledge" exists only as a pattern across billions of numbers, none of which individually contains it. (*Heredoc Manifesto*)