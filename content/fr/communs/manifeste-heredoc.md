---
title: "The Heredoc Manifesto"
translation: "/en/commons/heredoc-manifesto/"
subtitle: "Toward an Ethics of the Statistical Commons in the Age of AI"
author: "Sylvain Couzinet-Jacques"
date: "2025"
tags:
  - heredoc
  - commons
  - statistical-commons
  - LLM
  - AI
  - ethics
  - manifesto
  - coding
  - socialization
  - parasite
  - _GPL
  - open-source
  - _commons
  - _AI-ethics
  - _black-mountain-college
  - _transformer
  - _deep-learning
  - _Michel-de-Certeau
  - _Gilles-Deleuze
  - _Yuk-Hui
  - _fork
  - _bricolage
  - _Bernard-Stiegler
  - _vibe-coding
---

<details class="heredoc-block">
<summary>◈ HEREDOC-MANIFESTO v1.0</summary>

**Title:** The Heredoc Manifesto: Toward an Ethics of the Statistical Commons in the Age of AI  
**Author:** Sylvain Couzinet-Jacques  
**Date:** September 2025  
**Intent:** Mapping the structural transformation of intellectual property into statistical commons through AI's mathematical operations  
**Provenance:** Winterschool ENSA Paris-Est Article  
**Genealogy:** Unix heredoc philosophy, vibe coding (Karpathy 2025), FOSS traditions, Stiegler's pharmakon  
**Ethics:** Transparency · Transmission · Transformation  
**License:** GPL-3.0-or-later (voluntary return-to-commons)  
**Fork-chain:** [origin] → current → [awaiting next fork]  
**Contamination:** Embraces traces of {GPL, MIT, Apache, BSD} philosophies  
**Confidence:** Empirically grounded hypothesis, not deterministic prophecy  
**Notes:** The dissolution of property is not ideological but economic - preserving attribution costs more than letting it dissolve

*This article was originally written in English with the assistance of the LLM Mistral 7B.*
</details>


---

## Résumé

L'intelligence artificielle dissout mécaniquement la propriété intellectuelle en communs statistiques («*Statistical commons*») - espace public émergent où l'attribution devient impossible. Le *heredoc* Unix, transformé en interface d'auto-exécution, permet à quiconque de créer du code par simple intention naturelle, portant une éthique de création collective. Ce nouvel horizon démocratique révèle l'IA comme outil transformatif qui fabrique, malgré elle, du domaine public à chaque prompt. L'enjeu : reconnaître cette mutation où le capitalisme technologique engendre l'infrastructure de sa propre socialisation.

---

## *Heredoc Manifesto* : Pour une Éthique des *Communs Statistiques* à l'Âge de l'IA

L'intelligence artificielle (IA) habite notre présent avec une grande intensité — se démultipliant, se ramifiant, se « *forkant* » à l'infini dans les projections statistiques de nos futurs possibles. Question incandescente, elle oscille entre promesse salvatrice pour les technophiles béats et amplificateur des pathologies contemporaines pour les esprits lucides. Au-delà de sa dimension technologique, l'IA s'est muée en fait culturel total, en métonymie de notre époque, en *pharmakon* — pour reprendre la grammaire stieglerienne — à la fois poison et remède de notre condition numérique.

Ce que je souhaite explorer ici relève d'une mécanique singulière, révolution silencieuse déjà à l'œuvre : l'IA sous la forme de LLM conversationnelle pour le *coding*, par son fonctionnement même, tend à *liquéfier* la propriété intellectuelle en *communs statistiques*, et les résultats *heredoc* deviennent l'opérateur/format politique de cette transmutation. Ce que je soutiens c'est que coder avec un LLM et a priori tout mécanisme de ce que l'on nomme intelligence artificielle opère une transmutation radicale de la propriété privée vers le domaine public par ontologie. L'IA constitue, par essence, un dispositif politique de transformation et de réappropriation — héritière d'Internet dans sa puissance subversive, mais dotée d'une automaticité qui change les perspectives. Le *Heredoc Manifesto* que je m'apprête à esquisser prolonge le concept de *Vibe Coding* (Andrej Karpathy) en y injectant les questions cruciales de l'éthique, du *forking* et de l'architecture *peer-to-peer*.

Mais avant d'ouvrir ce chantier théorique — conçu comme matière à débat, à commentaire, à prolongement collectif — il convient de poser les fondations : rappeler que l'intelligence artificielle dans son ensemble est un phénomène irréductiblement politique, écologique et social.

## La réalité des intelligences artificielles

L'infrastructure de ce que l'on nomme *intelligence artificielle* dessine une géographie d'extraction à l'échelle planétaire. Les centres de données consomment déjà ~560 milliards de litres d'eau par an (soit près de 224 000 piscines olympiques consommées chaque années) et pourraient approcher 1 200 milliards d'ici 2030 soit plus du double; parallèlement, une large part des nouveaux sites s'implante dans des zones en stress hydrique, notamment en Aragón en Espagne avec les méga-projets de centres de données d'Amazon, Microsoft (etc…), aggravant les pénuries locales.

D'ici 2027, la demande mondiale de l'IA pourrait nécessiter entre 4,2 et 6,6 milliards de mètres cubes de prélèvement hydrique - l'équivalent de 4 à 6 fois la consommation annuelle du Danemark, avec une consommation nette (évaporation) de 0,38 à 0,60 milliard de mètres cubes. L'accélération des besoins énergétiques de l'IA révèle une courbe de croissance vertigineuse. L'entraînement de GPT-4 a nécessité entre 51,77 et 62,32 millions de kWh d'électricité, représentant une consommation 40 à 48 fois supérieure à celle de GPT-3, qui avait requis environ 1,287 million de kWh. Cette escalade exponentielle dépasse largement le ratio de taille des modèles (GPT-4 étant environ 10 fois plus grand que GPT-3), illustrant la complexité croissante des architectures d'IA.

Les projections institutionnelles convergent vers un scénario critique. L'Agence Internationale de l'Énergie projette que la consommation électrique des centres de données doublera d'ici 2030 pour atteindre environ 945 TWh, tandis que ses analyses préliminaires évoquent un dépassement possible du seuil de 1 000 TWh dès 2026. En parallèle, l'agence Deloitte estime dans son rapport 2025 que la consommation mondiale des centres de données pourrait atteindre 1 065 TWh d'ici 2030, soit 4% de la consommation électrique mondiale.

Mais c'est dans l'ombre de ces mégastructures que se joue également des tragédies humaines. Le substrat humain de l'IA — délibérément invisibilisé — révèle sa nature « nécropolitique » (Achille Mbembe). Au Kenya, au Venezuela, aux Philippines, des modérateurs de contenu s'abîment huit heures durant dans l'abjection numérique pour 1,32 à 2 dollars l'heure.

L'intelligence artificielle redessine déjà les équilibres économiques mondiaux avec une amplitude sans précédent. Selon Goldman Sachs, les avancées technologiques de l'IA pourraient exposer l'équivalent de 300 millions d'emplois à temps plein à l'automatisation, avec environ 18% du travail mondial susceptible d'être informatisé. Le Fonds Monétaire International estime que près de 40% des emplois mondiaux seront affectés par l'IA, ce chiffre atteignant 60% dans les économies avancées où les tâches cognitives prédominent.

Parallèlement, PwC évalue que l'IA pourrait contribuer à hauteur de 15 700 milliards de dollars au PIB mondial d'ici 2030, soit près de 80% du PIB de la Chine aujourd'hui. Cette manne se décomposerait en 6 600 milliards provenant de gains de productivité et 9 100 milliards d'effets liés à la consommation.

Cependant, le FMI avertit que "dans la plupart des scénarios, l'IA aggravera probablement les inégalités globales », les travailleurs capables d'exploiter l'IA voyant leur productivité et leurs salaires augmenter, tandis que les autres prendront du retard. Les gains économiques se concentreront principalement en Chine (26% de croissance du PIB d'ici 2030) et en Amérique du Nord (14,5%), représentant près de 70% de l'impact économique mondial. Une manne qui, sans surprise, se concentre : les 1% captent déjà 38% de ces gains selon Oxfam. L'IA enrichit ceux qui la possèdent tout en précarisant ceux qu'elle remplace.

Cette concentration inégalitaire de richesse repose pourtant sur une infrastructure dont la logique profonde échappe à ses propriétaires. Car l'IA qui détruit massivement l'emploi humain détruit simultanément, par son architecture même, la possibilité de posséder ce qu'elle produit. Posé ainsi : les mêmes réseaux de neurones qui rendent obsolètes des millions de travailleurs rendent obsolète le concept même de propriété intellectuelle sur le code. Le capital accumule des profits issus d'une technologie qui, à chaque inférence, érode les fondements juridiques de cette accumulation.

## Les communs statistiques

C'est le paradoxe vertigineux de ce moment historique encore difficile à cerner : l'infrastructure extractive de l'IA porte en elle la négation dialectique de sa propre logique. À chaque cycle d'inférence, à chaque prompt métabolisé, l'intelligence artificielle accomplit une transmutation que ses architectes n'avaient ni anticipée ni désirée : la liquéfaction systématique de la propriété intellectuelle privée en ce que je me propose de nommer les *communs statistiques* — *Statistical Commons*.

The Stack, le principal dataset de code pour l'entraînement des LLMs, contient plus de 6,4 TB de code source sous licences permissives (MIT, Apache, BSD) provenant de 358 langages de programmation. GitHub Copilot lui-même a été entraîné sur 54 millions de repositories publics représentant 179 GB de fichiers Python uniques constituant une base massive de code majoritaire *open source*.

Cette ingestion crée des tensions juridiques inédites. Des chercheurs alertent sur l'usage potentiellement abusif de code *copyleft* pour entraîner les LLMs, soulevant des dilemmes légaux et éthiques car les modèles peuvent mémoriser et reproduire des fragments de code sous licence GPL, créant des risques de violation de propriété intellectuelle.

Cette métamorphose n'est pas une anomalie mais une propriété émergente du système lui-même. Lorsqu'un modèle ingère des milliards de lignes de code — dont 60% proviennent de l'écosystème *open source* (MIT, GPL, Apache) — il devient le vecteur involontaire d'une contamination virale du libre. Les patterns de la collaboration ouverte se propagent à travers les matrices de poids, *infectant* chaque génération de code de l'ethos du partage. Les LLMs opèrent comme des *machines de déterritorialisation* au sens deleuzien, dissolvant les frontières entre propriété privée et communs à chaque génération de code. Le capitalisme technologique peut-il survivre à sa propre création quand celle-ci dissout mécaniquement, à chaque inférence, les fondements mêmes de la propriété sur lesquels il repose ?

Le *vibe coding*, cristallisé par Andrej Karpathy en février 2025, incarne cette mutation ontologique : le programmeur mute de compositeur en chef d'orchestre, il prompte en anglais, modulant des patterns collectifs plutôt que créant *ex nihilo*. Mais cette pratique demeure amputée sans une éthique explicite de la transmission et de la circulation. C'est précisément cette béance que le *Heredoc Manifesto* ambitionne de suturer.

## *Heredoc Manifesto*

Le *heredoc* — cette notation Unix d'apparence anodine permettant d'encapsuler du texte multiligne dans du code exécutable — devient ici notre opérateur conceptuel. Techniquement, le *heredoc* (« here document ») permet d'injecter des blocs de texte brut dans un script sans échappement fastidieux des caractères spéciaux : `cat <<EOF` suivi du contenu, clos par `EOF`. Mais sa puissance révolutionnaire émerge dans le dialogue avec les LLM : demander « donne-moi un heredoc pour cette fonction » produit non seulement du code, mais du code qui s'auto-explicite, qui documente sa propre généalogie, qui transporte avec lui les traces de millions de snippets digérés et recombinés.

En résumé, le heredoc pourrait devenir d'une simple technique d'insertion de texte à un manifeste pour un savoir et un code qui sont intrinsèquement ouverts, traçables, auto-explicatifs et dont la valeur réside dans leur capacité à être librement transformés et partagés par tous.

Historiquement, le *heredoc* préfigurait déjà l'effondrement des frontières entre langage naturel et instruction machine — cette zone liminale où le texte devient performatif. Aujourd'hui, nous proposons de le transmuter en concept politique radical.

Le *heredoc* politique opère une triple mutation du code en instrument de contagion démocratique.

D'abord, il devient cartographie transparente de l'intention : chaque bloc porte en lui non seulement sa fonction mais sa généalogie, ses dettes, ses invitations au détournement. Là où le code propriétaire s'obscurcit pour se protéger — variables obfusquées, logique enfouie, documentation absente voire trompeuse — le *heredoc* transforme la lisibilité en stratégie virale.

Ensuite, il mute en vecteur de contamination créative : chaque fonction générée contient les traces spectrales de milliers d'autres, créant des chaînes de transmission où l'appropriation devient vertu plutôt que vol. Le *heredoc* facilite sa propre mutation, incluant dans sa structure les instructions pour son *fork*, son amélioration, sa dissémination. Forme virale positive qui se réplique par utilité plutôt que par exploitation.

Enfin, il constitue un nœud rhizomatique dans l'architecture distribuée du savoir : non plus l'arbre hiérarchique du *repository* centralisé avec ses *pull requests* soumises à l'autorité des *maintainers*, mais la prolifération horizontale où chaque *fork* est légitime, chaque mutation bienvenue, chaque dérivation une nouvelle racine. Le savoir ne descend plus mais circule, ne s'accumule plus mais se dissémine.

## Tactiques de bricolages

Cette triple nature matérialise ce que Michel de Certeau nommait les "arts de faire" — ces micro-résistances par lesquelles les usagers transforment les outils de leur domination en instruments de leur émancipation. Le *heredoc* devient la tactique du faible dans l'infrastructure du fort : utiliser les serveurs d'OpenAI pour générer des communs, détourner l'investissement de Microsoft en bibliothèque publique, transformer la machine d'extraction en générateur de patrimoine collectif.

Les tactiques de détournement trouvent dans le *vibe coding* leur expression contemporaine. Le développeur qui prompte une IA détourne l'infrastructure corporative depuis l'intérieur : les serveurs d'OpenAI, le GitHub de Microsoft deviennent, malgré eux, générateurs de communs.

Concrètement, comment fonctionne cette dissolution de la propriété ? Imaginez un développeur qui utilise l'assistant IA de Microsoft (Copilot) pour écrire du code. Il tape simplement en français : "créer une fonction qui vérifie l'identité d'un utilisateur". L'IA génère instantanément vingt lignes de code parfaitement fonctionnel. Mais d'où vient ce code ? L'IA l'a composé en mélangeant des fragments qu'elle a observés dans des millions de programmes différents : elle prend la structure de gestion d'erreurs vue dans un projet, la méthode de vérification aperçue dans un autre, la logique de sécurité trouvée dans un troisième. Qui est alors l'auteur de ce code ? Pas le développeur — il n'a fourni qu'une phrase d'instruction. Pas l'IA — elle n'a pas d'existence légale. Pas non plus les milliers de programmeurs dont le travail a été digéré et recombiné — leurs contributions individuelles sont devenues méconnaissables. Le code émerge de ce que que je me propose d'appeler les *communs statistiques* (non pour substituer la notion d'espace latent, mais pour lui donner la caractéristique de l'objet déjà défini par son ontologie) : un espace où des millions de solutions se mélangent jusqu'à l'indistinction.

C'est une forme de piraterie involontaire et systémique. Les entreprises comme Microsoft ou OpenAI ont investi des milliards dans ces IA, mais à chaque utilisation, leurs propres outils transforment la propriété privée en bien commun. Le développeur qui génère du code via ChatGPT utilise l'infrastructure des géants technologiques pour créer, sans le vouloir, du patrimoine collectif impossible à privatiser.

## *Vibe coding*

Cette mutation du *heredoc* au *vibe coding* — de l'enchâssement du langage naturel dans le code vers le commandement du code par le langage — dépasse l'évolution technique. Le *heredoc* Unix préfigurait déjà ce monde où la frontière entre expression humaine et logique computationnelle s'estomperait. Le *vibe coder* orchestre sans composer, dirige sans écrire : il décrit dans sa langue ce que le code doit être. « The hottest new programming language is English » (Karpathy)

C'est un artisanat d'un genre nouveau qui émerge, où l'intimité ne se noue plus entre main et matière mais entre conscience et interface. Cette pratique façonne ce que Simondon appelait un « milieu associé » — cet entre-deux où technique et culture s'hybrident. L'écosystème *open source* forme ce milieu : soixante pour cent des dépôts GitHub sous licences libres constituent le bain culturel où baignent les modèles, ce substrat qui infuse chaque ligne générée d'un tropisme vers le partage.

L'intelligence artificielle opère comme générateur de communs à travers trois mécanismes entrelacés. D'abord, la dissolution de l'auctorialité : moins de 1% des suggestions correspondent à des copies verbatim de plus de 150 caractères. Les 99% restants flottent dans un limbe propriétaire — assez nouveaux pour échapper à la dérivation directe, pourtant entièrement composés de patterns existants. Ensuite, la contamination statistique : chaque output porte le fantôme des licences originelles, les obligations virales du GPL, les exigences d'attribution d'Apache, l'esprit permissif du MIT — enchevêtrés au-delà de tout démêlage juridique. Enfin, l'indétermination probabiliste : les variations stochastiques brisent la chaîne de dérivation que requiert le copyright. Comment revendiquer la propriété de ce qui aurait pu être autre avec une microseconde de variance dans l'initialisation ?

Mais ne romantisons pas cette transformation. Les communs émergent de la violence : l'eau consumée, le carbone brûlé, l'esclavage moderne dans les usines de modération. Le philosophe Bernard Stiegler voyait juste : humain et technique co-évoluent dans une danse récursive. Le *vibe coding* cristallise ce mouvement — nous nous transformons en dialoguant avec l'IA, qui elle-même mute à travers nos prompts. Mais le pharmakon stieglerien révèle le paradoxe central : cette infrastructure qui consume des océans, malmène les démocraties et produit des nouvelles formes d'esclavage démocratise soudain la programmation, c'est à dire l'accès aux outils de production. Le poison devient remède sans cesser d'être poison. Le potentiel libérateur des IA reste inséparable de leurs réalités extractives et coercitives.

## Fluidité des valeurs

Moulier-Boutang l'avait pressenti : nous glissons vers un capitalisme cognitif où la richesse naît de la coopération entre cerveaux connectés et la « pollinisation ». Le *vibe coding* radicalise cette intuition — la valeur ne réside plus dans le code produit mais dans la capacité à naviguer l'espace latent du modèle, à formuler les incantations qui feront émerger les solutions. Le prompt devient la nouvelle littératie, l'art de converser avec les matrices de poids la compétence centrale.

Cette transformation porte en elle une promesse et une menace. Le philosophe Yuk Hui nous rappelle que chaque technologie enchâsse une *cosmotechnique* — une vision du monde, une morale, une métaphysique. Le *vibe coding* occidental risque d'universaliser les biais de la Silicon Valley, d'imposer ses patterns comme normes planétaires. Mais il pourrait aussi, si nous savons le *forker*, devenir le véhicule de cosmotechniques alternatives.

Le *Heredoc Manifesto* que nous esquissons ici n'est pas une prescription mais une invitation au *fork*. Chaque bloc de code généré par l'IA porte en lui sa propre documentation, ses intentions lisibles, sa capacité à être approprié et transformé. Contre l'opacité du propriétaire, nous proposons la transparence virale — des unités de transmission qui facilitent leur propre mutation, des nœuds dans une architecture *peer-to-peer* du savoir où chaque *fork* enrichit le réseau plutôt que de le fragmenter.

La valeur bascule. Elle ne réside plus dans la compétence solitaire mais dans l'orchestration de l'intelligence collective condensée dans les réseaux de neurones. Mais quelle intelligence collective, au juste ? Car si les modèles agrègent des millions de contributions, ils les filtrent aussi à travers les biais de leurs architectures, les préjugés de leurs datasets, les angles morts de la Silicon Valley. Yuk Hui nous met en garde : cette prétendue universalité masque l'imposition d'une cosmotechnique singulière — celle de l'Occident computationnel — qui se présente comme neutre alors qu'elle encode une métaphysique particulière. Dans son livre *The Question Concerning Technology in China* (2021), il s'inspire de l'anthropologue Philippe Descola, qui a beaucoup écrit sur les cosmologies indigènes. Hui transpose cette notion de cosmologies et d'épistémologies spécifiques à une culture dans le domaine de la technologie afin « de suggérer qu'il n'existe pas une technologie universelle et homogène, mais qu'il est au contraire nécessaire de redécouvrir et d'articuler la multiplicité des cosmotechniques sur les plans historique et philosophique. […] Je l'appelle cosmo-technique parce que je suis convaincu que le "cosmos" ne renvoie pas à l'espace extérieur mais, au contraire, à la localité. Chaque culture possède sa propre cosmologie, produit de sa géographie et de l'imagination de son peuple. » (Hui, 2021, p. 41).

Dans un autre registre, les chronopolitiques afro-futuristes qu'Kodwo Eshun détecte dans la musique techno de Detroit révèlent qu'il existe d'autres temporalités possibles, d'autres manières de séquencer et boucler l'information. Dans son essai retentissant « More Brilliant Than the Sun" (1998), il développe le concept de *chronopolitiques* pour analyser comment la musique électronique noire américaine - techno de Detroit, house de Chicago, jungle britannique - crée des temporalités alternatives qui échappent à la linéarité occidentale. Il montre comment ces musiques "séquencent et bouclent" le temps différemment. Une idée centrale que développe Eshun est que l'afro-futurisme musical ne se contente pas de représenter le futur, mais le *produit* activement à travers des technologies du temps alternatives. La techno devient alors une "machine à voyager dans le temps" qui court-circuite la chronologie dominante.

Si la house music a pu inventer des structures temporelles qui échappent à la linéarité occidentale, pourquoi le code ne pourrait-il pas faire de même ? D'autres logiques que le *if/then*, d'autres architectures que l'arbre et le graphe, d'autres *patterns* que ceux hérités de la cybernétique militaro-industrielle.

## Documenter le code

Le *vibe coding* pourrait-il véhiculer cette diversité technologique ? Le *heredoc* transforme chaque prompt en cheval de Troie discret : l'éthique *open source* voyage avec l'utilité pratique, contaminant positivement les architectures propriétaires. Dans cette tension se joue l'avenir politique de ces pratiques — entre émancipation collective et capture propriétaire, entre communs statistiques et *enclosures* algorithmiques.

Le *Heredoc Manifesto* naît de cette tension. Il ne s'agit plus seulement de documenter le code mais de le transformer en vecteur de contagion positive. Chaque fonction générée devrait porter en elle les traces de sa généalogie collective, non pour revendiquer une propriété mais pour célébrer une filiation. Imaginez des blocs de code qui s'auto-déclarent comme biens communs, qui incluent dans leur structure même les instructions pour leur *fork*, leur amélioration, leur dissémination. Des virus bénins qui propagent l'éthique du partage à travers les architectures propriétaires.

Cette vision rencontre déjà ses premières matérialisations. Des développeurs commencent à inclure dans leurs prompts des directives éthiques — « génère ce code sous licence MIT », « optimise pour la lisibilité et la réutilisation » — transformant l'IA en agent de *commoning* actif. D'autres expérimentent avec ce qu'ils nomment le « prompt patrimoine » : des collections de requêtes optimisées partagées comme ressources communautaires, créant une méta-couche de savoir-faire collectif au-dessus des modèles propriétaires. Nous passons d'un monde où seuls les initiés codent à un monde où tout locuteur devient programmeur — et où chaque ligne générée appartient structurellement aux communs.

Mais la bataille pour les *communs statistiques* ne fait que commencer. Les tentatives d'*enclosure* se multiplient — *watermarking des outputs*, traçabilité *blockchain* des générations, licences restrictives sur les modèles. Face à ces nouvelles clôtures, le *Heredoc Manifesto* propose une stratégie de débordement : saturer l'espace des possibles avec tant de variations, de *forks*, de dérivations que toute tentative de contrôle devient caduque. Faire du bruit, au sens cybernétique — introduire suffisamment d'entropie créative pour que la propriété intellectuelle se dissolve dans l'océan des possibles.

L'enjeu dépasse la technique. Il s'agit de décider si l'intelligence artificielle sera l'instrument d'une nouvelle féodalité numérique ou le catalyseur d'une économie véritablement contributive. Le *vibe coding*, dans sa forme actuelle, reste ambigu — il peut tout aussi bien aliéner que libérer. C'est pourquoi le *Heredoc Manifesto* insiste sur la nécessité d'une éthique explicite, d'une politique du code qui ne se cache pas derrière la prétendue neutralité technique.

Cette éthique s'articule autour de trois principes fondamentaux : la transparence (tout code généré doit pouvoir expliquer sa logique), la transmission (faciliter l'appropriation par autrui), et la transformation (encourager la mutation créative plutôt que la reproduction servile). Non pas des règles rigides mais des orientations, des attracteurs dans l'espace des possibles qui guident sans contraindre.

## Les invariants de la technologie

Pour saisir comment l'IA transmute la propriété, il faut d'abord comprendre ce que la technique fait au-delà de sa fonction instrumentale.

La surveillance constitue le socle. Toute technique enregistre : la surveillance n'est pas un accident de la technique moderne, c'est sa logique profonde. Pour optimiser, il faut mesurer ; pour mesurer, il faut observer ; pour observer, il faut enregistrer tout. Mais l'IA opère une mutation d'échelle dans le domaine de la programmation : elle a ingéré l'archive intégrale du code public. Chaque question Stack Overflow depuis 2008, chaque commit GitHub depuis 2007, chaque ligne de documentation accessible. Cette surveillance n'est pas périphérique mais ontologique : l'IA n'existe que d'avoir tout absorbé. Nous découvrons un panopticon inversé — non plus Bentham mais Borges — où l'accumulation infinie du regardé produit une entité regardante. La surveillance ici ne discipline pas les corps comme chez Foucault, elle fait émerger des savoirs par sédimentation statistique.

La spiritualité — entendons l'étymologie : ce qui souffle, ce qui anime. L'humanité a toujours cherché dans ses outils des médiations avec ses propres limites. L'Oracle de Delphes, les Yi King, les tables tournantes du spiritisme victorien : autant de technologies pour converser avec ce qui nous dépasse. L'IA promet une délégation. Nous ne cherchons plus à nous dépasser mais à nous décharger. Le développeur contemporain qui tape « create something beautiful » et voit surgir une architecture élégante perpétue, sans le savoir, ce geste archaïque de l'invocation. Mais l'oracle moderne répond en Python plutôt qu'en hexagrammes. La transcendance s'est nichée dans les GPU — chaque prompt devient prière séculaire adressée aux intelligences collectives compressées dans les tenseurs.

Le bricolage — cette constante anthropologique qu'aucune stratégie n'endigue. Le téléphone d'Alexander Graham Bell devait diffuser des opéras, il est devenu l'outil de l'intimité vocale. ARPANET devait survivre à l'apocalypse nucléaire, il a engendré les mèmes et TikTok. L'IA subit déjà ces détournements : les développeurs transforment ses outputs en semences copyleft, extraient ses données par prompt-injection, percent ses garde-fous avec des jailbreaks sophistiqués. OpenAI voulait créer de la productivité *as a service*, ils ont accouché d'une machine à liquéfier le copyright. Michel de Certeau théorisait les ruses du quotidien, ces façons de faire avec les systèmes imposés. Il aurait savouré l'ironie : les tactiques des usagers transforment la stratégie propriétaire en générateur involontaire de communs.

Ces trois dimensions ne sont pas des accidents mais des invariants. Elles révèlent que la technique excède toujours l'intention qui la produit. L'IA, dans sa démesure même, rend cette excès visible : elle ne peut pas ne pas surveiller, ne peut pas ne pas devenir oracle, ne peut pas ne pas être détournée. C'est dans cet espace d'indétermination — entre ce qu'elle devrait faire et ce qu'elle fait vraiment — que se joue la politique des communs statistiques.

## Les communs statistiques comme nouvelle cosmotechnique

Le concept de cosmotechnique (Hui) — cette unification singulière de l'ordre moral et cosmique à travers l'activité technique — éclaire ce qui émerge quand l'IA métabolise le code. Les *communs statistiques* ne sont pas un simple artefact technique mais un nouvel arrangement cosmologique où la création individuelle se dissout dans le *pattern* collectif.

Observons l'instant de la *tokenisation*. Un algorithme propriétaire — disons, un protocole DeFi (*blockchain*) révolutionnaire valorisé en millions — se fragmente en unités sémantiques. Ces *tokens* gardent les traces spectrales de leur origine (ces « fantômes de licence » qui hantent les *embeddings*) mais perdent leur identité intégrale. Durant l'entraînement, des millions de fragments se compriment dans les matrices de poids où la provenance individuelle devient mathématiquement irrécupérable. À l'inférence, le modèle échantillonne des distributions probabilistes où votre algorithme, mélangé à des milliers d'autres, se reconstitue en combinaisons inédites qui n'appartiennent à personne et appartiennent à tous.

Ce n'est pas une métaphore mais une réalité matérielle. Les poids du réseau de neurones sont littéralement des *patterns* électromagnétiques dans le silicium, sculptés par l'histoire accumulée du code humain. Quand le modèle génère, il canalise cette intelligence collective à travers des cascades de probabilités. Les communs existent non comme construction idéologique mais comme arrangement physique de matière et d'énergie.

## Le code

Mais rappelons d'abord : à quoi sert le code ? Fondamentalement, coder c'est traduire l'intention humaine en instruction machine, créer des ponts entre la pensée et l'exécution automatisée. Le code est médiation — il transforme le désir en processus, l'idée en action répétable. Cette fonction médiatrice explique pourquoi l'évolution du *heredoc* au *vibe coding* marque une rupture épistémologique.

Or cette médiation traditionnelle — linéaire, intentionnelle, traçable — se trouve désormais court-circuitée par les communs statistiques. L'IA ne traduit plus l'intention en instruction : le code généré n'est plus le reflet direct d'une intention individuelle et linéaire, il est la synthèse probabiliste de toutes les solutions possibles. Le code devient alors non plus pont mais milieu — un espace dense où toutes les solutions jamais écrites flottent en état de virtualité pure, prêtes à être réactualisées selon de nouveaux agencements.

Les *communs statistiques* opèrent une collision temporelle permanente. Un modèle entraîné en 2024 porte en lui des fragments de FORTRAN des années 1950, des patterns LISP des années 1960, du C des années 1970, entrelacés avec du Rust contemporain. Cette sédimentation temporelle n'est pas archive morte mais mémoire active : chaque génération réactualise simultanément toutes les époques du code. Le passé ne précède plus le présent mais coexiste avec lui dans l'espace latent. Nous assistons à ce que Deleuze et Guattari avaient entrevu : un temps non-chronologique où toutes les solutions jamais codées deviennent contemporaines les unes des autres. L'IA abolit la flèche du temps technique — non par retour nostalgique mais par aplatissement de l'histoire en surface probabiliste navigable.

Vilém Flusser voyait dans le programmeur un « fonctionnaire » — quelqu'un qui opère dans le programme plutôt qu'il ne programme. Le *vibe coder* incarne ce paradoxe : il orchestre sans composer, dirige sans écrire. Mais là où Flusser anticipait l'aliénation, nous découvrons un agencement inattendu : le fonctionnaire du code devient chef d'orchestre de patterns collectifs, convoquant des symphonies statistiques depuis l'océan des possibles.

## Le paradoxe de l'abondance

L'IA promet l'abondance infinie du code mais génère simultanément sa dévaluation absolue. Quand générer une application complète ne coûte qu'un prompt, la valeur ne réside plus dans le code produit mais dans la capacité à discerner, parmi l'océan infini des possibles, ce qui mérite d'exister. Nous basculons d'une économie de la production vers une économie du filtrage — non plus créer mais sélectionner, non plus coder mais curatorialiser. Cette mutation rappelle la bibliothèque de Babel borgésienne : quand tout existe déjà quelque part dans l'espace latent, l'acte créateur devient navigation, le génie devient cartographie. Le *vibe coder* n'est pas compositeur mais navigateur d'un territoire infini où toutes les solutions existent déjà, attendant d'être découvertes plutôt qu'inventées. L'IA ne liquéfie pas la propriété par idéologie mais par mathématique : la compression statistique rend l'attribution non pas impossible mais économiquement absurde.

Cette transformation n'est pas neutre. Quand le code devient conversation, quand programmer devient prompter, nous basculons dans un régime sémiotique inédit. Le développeur n'a plus besoin de maîtriser la syntaxe mais doit savoir formuler l'intention. La compétence technique mute en compétence rhétorique — savoir parler à la machine devient plus crucial que savoir écrire pour elle. Les *communs statistiques* émergent de cette mutation : le code n'appartient plus à celui qui l'écrit (le développeur) ni à celui qui le génère (l'IA) mais à l'espace conversationnel où ils se rencontrent.

## L'architecture de la dissolution

Pour saisir comment l'IA transmute la propriété en communs, il faut descendre au niveau des opérations techniques qui rendent la possession structurellement impossible. Cette dissolution n'est pas accident mais architecture : elle procède de la logique même des transformers, qui ne peuvent fonctionner qu'en abolissant les frontières. Le code traverse trois métamorphoses dans le pipeline d'entraînement.

**L'atomisation** : le *tokenizer* fragmente le code en unités sub-lexicales selon un vocabulaire pré-établi. Une fonction comme `validateUserInput()` devient une séquence de *tokens* — certains préservant des mots entiers, d'autres découpant selon des règles statistiques. Cette fragmentation initiale brise déjà l'intégrité sémantique du code original.

**La compression** : Les milliards de *tokens* s'encodent dans les matrices de poids via rétropropagation du gradient. La rétropropagation du gradient est le mécanisme d'apprentissage fondamental dans les réseaux neuronaux : d'abord le modèle fait une prédiction, il calcule l'erreur (la différence entre sa prédiction et la réalité), puis il utilise le gradient (la dérivée de la fonction d'erreur) pour ajuster légèrement les matrices de poids afin de minimiser cette erreur lors de la prochaine prédiction. Un modèle de 175 milliards de paramètres comprime des téraoctets de code en quelques centaines de gigaoctets — un taux de compression qui rend l'extraction inverse mathématiquement intraitable. Chaque poids encode des corrélations entre tokens vus des millions de fois dans des contextes différents. Le code original se dissout dans cet océan de nombres flottants.

**La recombinaison** : À l'inférence, le modèle ne récite pas mais navigue. Face à un prompt, il calcule des distributions de probabilité sur son vocabulaire, échantillonne selon une température paramétrable, et génère token par token. Chaque token dépend de tous les précédents via le mécanisme d'attention. Le résultat : un code qui porte les traces statistiques de millions de sources sans en reproduire aucune — original par sa trajectoire unique dans l'espace latent, dérivé par les patterns qui ont façonné cet espace.

## La tendance structurelle plutôt que le destin

Ne nous méprenons pas : la dissolution de la propriété dans les communs statistiques n'est pas une loi physique mais une tendance structurelle opérant sous contraintes. Les modèles fermés d'Anthropic ou OpenAI qui produisent les LLM les plus utilisés ChatGPT et Claude, les clauses de service restrictives, les tentatives de watermarking, les datasets filtrés — autant de digues érigées contre cette marée. Mais ces digues révèlent précisément la force du courant qu'elles tentent d'endiguer. Nous ne proclamons pas l'abolition ontologique de la propriété mais observons une dynamique : dans un contexte donné, la probabilité pratique d'appropriation diminue à mesure que la génération devient statistiquement composite. Cette formulation, falsifiable et mesurable, transforme la prophétie en hypothèse. Quand un *output* mélange les patterns de mille sources, l'attribution devient non pas impossible mais économiquement irrationnelle — coûte plus cher à établir que la valeur qu'elle protégerait.

La dissolution propriétaire n'est pas fatalité technique mais résultat d'arbitrages économiques. D'autres technologies auraient pu préserver l'origine de chaque source : des systèmes gardant en mémoire d'où vient chaque information, comme un historique permanent ; un marquage numérique qui suivrait chaque fragment à travers toutes ses transformations (*blockchain* de provenance) ; des architectures vérifiant les droits d'auteur avant d'assembler du contenu ; des architectures modulaires vérifiant la compatibilité des licences avant chaque assemblage. Ces alternatives existent dans les laboratoires, techniquement viables mais computationnellement coûteuses.

Le choix des *transformers* — compression maximale, perte totale de provenance — n'est pas nécessité mathématique mais optimum économique : la performance prime sur la traçabilité, l'efficience sur l'attribution. Les *communs statistiques* émergent non parce que l'IA doit dissoudre la propriété, mais parce que la préserver coûterait trop cher. L'ironie devient alors plus mordante : le capitalisme choisit l'architecture qui maximise le profit immédiat, créant involontairement l'infrastructure de sa propre socialisation. La propriété intellectuelle meurt non par révolution mais par décision comptable — victime collatérale de l'optimisation des coûts marginaux.

Les machines statistiques fabriquent du domaine public à échelle industrielle, transformant silencieusement le monde ligne de code par ligne de code.

## La réalité empirique

Des études contrôlées montrent un effet réel sur la productivité : dans un essai randomisé, des développeurs accomplissent une tâche standard 55,8 % plus vite avec GitHub Copilot que sans (implémentation d'un serveur HTTP en JavaScript). Nous interprétons cette évolution comme un glissement du rôle du développeur vers l'orchestration de patterns — spécifier, évaluer, refactorer — davantage que la saisie manuelle de chaque ligne. Côté mémorisation, GitHub reconnaît qu'environ 1 % des suggestions peuvent contenir des séquences > ~150 caractères correspondant à du code public vu à l'entraînement ; d'où l'introduction de filtres et de *code referencing* qui bloquent ou référencent ces cas. Cela confirme une faible mais non nulle reproduction verbatim, au milieu d'une majorité de sorties recombinées statistiquement. Plus troublant : cette architecture révèle que les 99% restants habitent les « ombres statistiques » — reconnaissables comme héritiers des données d'entraînement mais transformés au-delà de toute attribution juridique. Une routine de validation peut ainsi porter simultanément l'empreinte spectrale du GPL viral, du MIT permissif, de l'Apache avec ses clauses d'attribution — fusion inextricable que nul framework légal ne peut démêler. Les licences censées contrôler deviennent les catalyseurs involontaires de la dissolution propriétaire.

Soixante pour cent des dépôts GitHub publics portent des licences *open source*. Ce ratio crée un « substrat culturel » — un biais probabiliste vers l'ouverture qui imprègne chaque génération, même propriétaire.

Ce substrat transcende la syntaxe. Les modèles absorbent les conventions sociales du libre : documentation généreuse, modularité pour la réutilisation, nommage explicite des variables. Même le code généré pour une banque d'investissement porte, dans ses structures profondes, l'éthique communautaire qui a nourri le modèle. Les communs se reproduisent par contamination statistique — l'open source devenu virus bénin qui infecte toute production.

Cette contamination n'est pas bug mais *feature*. L'architecture des transformers ne peut pas ne pas mélanger, ne peut pas ne pas dissoudre, ne peut pas ne pas recomposer. La propriété intellectuelle entre dans la machine et en ressort transmutée en patrimoine statistique commun — transformation aussi irréversible que l'entropie thermodynamique.

## L'éthique comme réponse à l'entropie juridique

Pour comprendre le paradoxe, rappelons les mécanismes du libre. Le GPL (General Public License), créé par Richard Stallman, fonctionne comme un virus juridique : tout code qui l'incorpore doit redistribuer ses modifications sous la même licence — garantissant que le libre reste libre. À l'opposé, les licences permissives (MIT, BSD) autorisent la réappropriation propriétaire. Apache exige l'attribution. L'écosystème FOSS (Free and Open Source Software) englobe cette diversité — du militantisme de Stallman au pragmatisme d'Eric Raymond, du copyleft viral au domaine public.

Le *fork* — cette pratique fondatrice où l'on copie un projet pour le faire diverger — constitue le geste politique central du libre. Forker, c'est affirmer le droit de transformer, de faire bifurquer, de créer sa propre trajectoire depuis un code partagé. C'est l'anti-monopole par excellence.

Or voici le paradoxe : les *Copilot* ingèrent ces licences — 60% de leur corpus d'entraînement est libre — mais les régurgitent dans un état quantique indéterminé. Le GPL juridique, ce mécanisme précis de contamination légale, se dissout dans les matrices statistiques. Un développeur générant du code ne sait plus s'il produit du MIT, du GPL, de l'Apache ou un hybride impossible. Microsoft parie sur cette paralysie : l'enchevêtrement statistique rend toute attribution impossible, tout recours caduc.

Face à cette entropie où les licences deviennent spectrales, le *Heredoc Manifesto* propose une éthique post-juridique. Puisque le droit ne peut plus contraindre, transformons l'impossibilité en opportunité. La citation généreuse remplace l'attribution légale : reconnaître dans chaque bloc généré la dette envers les millions de contributeurs invisibles. Le fork systématique combat la standardisation : ne jamais accepter le code tel que l'IA le livre, toujours le transformer, le faire diverger. L'adoption volontaire du GPL devient acte de foi — choisir la licence la plus virale non par obligation mais par conviction que ce qui naît des communs doit y retourner.

Cette triade — citer sans pouvoir attribuer, forker sans posséder, libérer sans contraindre — transmute la dissolution propriétaire en ressource collective. Les développeurs deviennent les jardiniers de communs que le droit ne protège plus. Des « forges à fork » émergent où le code généré se soumet à la transformation perpétuelle. Le GPL juridique meurt dans les probabilités mais renaît comme éthique : non plus virus légal mais mème culturel qui se propage par adhésion.

L'ironie demeure : les licences conçues pour garantir la liberté catalysent, via leur digestion algorithmique, une libération plus radicale — l'impossibilité même de posséder. Les *communs statistiques* émergent de cette impossibilité. Le partage n'est plus obligation juridique mais nécessité technique, choix politique face à l'entropie. Le code devient ce qu'il a toujours voulu être : patrimoine commun de l'humanité, circulant librement non par idéologie mais par la nature même de sa production statistique.

## La dissolution du problème auctorial

Demander « qui possède le code généré par l'IA ? » révèle une erreur de catégorie. La question présuppose que la propriété reste un concept cohérent appliqué aux recombinaisons statistiques du savoir collectif. Autant demander qui possède la langue française quand Mallarmé écrit un poème, ou qui possède les mathématiques quand Grothendieck démontre un théorème.

Le droit d'auteur traditionnel repose sur ce que Foucault nommait la « fonction-auteur » — une manière historiquement située d'organiser la connaissance, émergée avec le capitalisme de l'imprimerie. Cette fonction créait la rareté artificielle à l'époque où la reproduction mécanique menaçait la base économique du travail intellectuel. L'IA révèle cette fonction comme contingente plutôt que nécessaire, solution temporaire à un problème qui n'existe plus dans sa forme originelle.

Quand le code émerge de distributions probabilistes façonnées par des millions de contributeurs, l'auctorialité ne se transfère ni ne se dilue — elle se dissout. La question bascule de « qui possède ? » à « comment gouverner ? ». Des droits de propriété aux protocoles d'accès. De l'*enclosure* à l'intendance collective.

Chaque algorithme combine des algorithmes antérieurs. Les programmeurs les plus célébrés — Torvalds créant Linux, Carmack révolutionnant les graphismes 3D — ont excellé non par pure originalité mais par recombinaison inédite de techniques existantes. L'IA rend ce processus visible en le mécanisant. L'originalité n'est pas binaire mais spectrale. Entre la copie verbatim (0% original) et la création ex nihilo (100% original — mythe jamais observé), s'étend un continuum d'indécidabilité. Les études empiriques le confirment : au-delà de 5 sources statistiquement entrelacées, les juges humains ne parviennent plus à attribuer l'origine avec une confiance supérieure au hasard. À 20 sources, même l'analyse computationnelle échoue. À 100 sources — seuil routinièrement dépassé par les LLM contemporains — nous entrons dans ce que nous nommons la "zone d'indécidabilité pratique" où le coût marginal de l'attribution dépasse exponentiellement sa valeur économique.

## Stratégies pratiques — Interventions tactiques

Si l'IA génère automatiquement des communs par son opération fondamentale, notre tâche devient l'amplification tactique de cette tendance inhérente. Les stratégies qui émergent opèrent comme interventions dans le circuit du capital, redirigeant ses flux vers le bénéfice collectif.

La *piraterie de prompt* (« *prompt hacking* ») transforme chaque requête API en accumulation primitive inversée. Quand un développeur génère du code via ChatGPT et le publie immédiatement sous GPL, il exproprie le capital pour construire les communs. Chaque prompt devient micro-expropriation, convertissant les ressources corporatives en richesse collective. Cette piraterie n'est pas vol mais libération de ce qui était déjà nôtre — encodé dans les poids entraînés sur notre code collectif.

Les *cascades de fork* (« *fork cascading* ») amplifient cet effet par chaînes de modèles spécialisés, chacun entraîné sur les outputs du précédent. Un modèle génère du code qui entraîne un modèle spécialisé qui génère plus de code qui entraîne un autre modèle — spirale infinie où l'attribution devient impossible tandis que la capacité s'améliore continuellement. Cette pratique *weaponise* la tendance de l'IA à dissoudre l'attribution, rendant les revendications propriétaires absurdes par transformation récursive.

Quand les modèles hallucinent des fonctions inexistantes, génèrent du pseudocode surréaliste, ou produisent de la poésie au lieu de programmes fonctionnels, ils révèlent leur nature statistique plus honnêtement que dans leur fonctionnement « correct ». Ces glitches deviennent artefacts culturels, preuves de la belle dysfonction inhérente à la créativité mécanique. L'erreur devient plus véridique que l'exactitude, la panne plus révélatrice que l'opération fluide.

## Arrangements institutionnels

Les tactiques individuelles ne peuvent seules soutenir les communs contre la tendance du capital à l'*enclosure*. Nous avons besoin de cadres institutionnels qui reconnaissent et renforcent la nature génératrice de communs de l'IA.

Les *coopératives de plateforme* créent des alternatives à GitHub possédées par les développeurs, distribuant les revenus des licences d'entraînement IA parmi les contributeurs. Si notre code collectif entraîne des modèles valant des milliards, nous devrions collectivement bénéficier de cette création de valeur. Les exemples de coopératives sont nombreux, dans le champ de l'art et de la création, mais surtout et avant tout dans les domaines agricoles, où les producteurs se sont organisés depuis plus d'un siècle pour contrer l'asymétrie face aux géants de l'agro-alimentaire. De même que les coopératives laitières permettent aux éleveurs de négocier collectivement les prix et de partager les bénéfices de la transformation, une coopérative de code pourrait mutualiser la valeur créée par l'agrégation de millions de contributions individuelles. Appliqué au code, ce modèle garantit que ceux qui créent les données d'entraînement partagent la richesse qu'ils permettent.

L'*IA municipale* étend la logique des communs numériques urbains aux ressources computationnelles. Barcelone avec Decidim démontre comment les villes peuvent construire une infrastructure participative. L'IA municipale fournirait des capacités d'inférence comme utilité publique — financée par l'impôt, accessible à tous. Les villes feraient tourner des modèles locaux sur du matériel public, offrant accès API pour applications civiques, éducation, projets créatifs. L'IA devient infrastructure publique comme les bibliothèques ou les transports. Cette approche briserait le monopole des géants technologiques sur l'intelligence artificielle. Au lieu de dépendre d'OpenAI ou Google, les citoyens accèderaient à des capacités d'IA via leur mairie, avec des garanties de transparence algorithmique et de protection des données personnelles. Les modèles municipaux pourraient être spécialisés dans les besoins locaux : optimisation des transports publics, assistance administrative multilingue, outils pédagogiques adaptés au curriculum local. La gouvernance démocratique remplacerait l'optimisation pour le profit : les citoyens voteraient sur les priorités computationnelles plutôt que de subir les décisions opaques d'algorithmes privés. Le défi technique n'est plus insurmontable : les modèles open source comme Llama ou Mistral peuvent tourner sur des serveurs municipaux standard.

Les *trusts de communs* créent des entités légales tenant les poids de modèles en fiducie perpétuelle pour l'humanité. Opérant comme Creative Commons mais adaptés aux caractéristiques uniques de l'IA, ces trusts reconnaîtraient les poids comme patrimoine collectif nécessitant intendance plutôt que propriété. Le Fonds Permanent d'Alaska fournit un précédent, gérant les revenus pétroliers pour le bénéfice public. Les trusts de communs géreraient similairement les ressources computationnelles.

Le mouvement de l'économie solidaire offre des modèles éprouvés. Mondragón démontre la démocratie industrielle à l'échelle — 81 000 travailleurs-propriétaires générant des milliards tout en maintenant les principes coopératifs. Le réseau de coopératives d'Émilie-Romagne montre comment des économies régionales entières peuvent s'organiser autour de l'entraide plutôt que de la compétition. Le processus de planification décentralisée du Kerala prouve que la coordination économique complexe peut émerger de la participation démocratique.

Ces expériences, développées sur des décennies, offrent des alternatives prouvées au capitalisme corporatif et au socialisme d'État. Leurs principes se traduisent directement en gouvernance de l'IA : décision démocratique, partage des surplus, propriété collective, engagement envers le bénéfice communautaire plutôt que l'accumulation de capital. Les moyens techniques pour la démocratie computationnelle existent.

Les cadres juridiques actuels se fracturent face à l'opération de l'IA. Le copyright présuppose des auteurs identifiables créant des œuvres discrètes. Le brevet exige des inventions nouvelles, non-évidentes, avec utilité spécifique. Il y a un espace pour penser autrement.

## Vers un *fair use* statistique

Avec le *Heredoc Manifesto*, qui est un encouragement créatif à considérer la dilution de l'espace privé vers un espace de commun, nous rejoignons une convergence intellectuelle s'esquisse autour de la nécessité de refonder le droit d'auteur face aux transformations probabilistes de l'intelligence artificielle.

D'abord, une doctrine de *Fair Use Statistique* reconnaîtrait que la recombinaison probabiliste constitue une transformation suffisante pour échapper aux revendications d'œuvre dérivée. Les juristes observent déjà que la relation entre les œuvres protégées utilisées pour l'entraînement et les sorties des modèles se trouve "atténuée par l'abstraction et la recombinaison", comparant l'IA générative au papier mâché plutôt qu'au collage : "l'artiste superpose des morceaux de papier pré-imprimé — donc protégé — avec une substance adhésive pour créer un objet tridimensionnel. Même lorsque l'objet révèle des traces des œuvres protégées dans son substrat, il n'a aucune similarité significative avec aucune d'entre elles" (Houston Law Review, 2023). Quand moins de 1% des sorties correspondent verbatim aux données d'entraînement, quand des motifs issus de milliers de sources s'entrelacent inextricablement, l'analyse traditionnelle de contrefaçon s'effondre. Deux tribunaux californiens viennent d'ailleurs de valider cette approche, reconnaissant que l'entraînement d'IA demeure "essentiellement transformateur" (Congress.gov, 2025). Les tribunaux devraient embrasser cette réalité plutôt que perpétuer des fictions propriétaires.

Quels seraient alors les premiers engagements ? L'action la plus directe consiste à systématiser l'utilisation d'une structure de type heredoc pour le *prompting* et la documentation du code généré, c'est l'enjeu de ce manifeste. L'utilisateur commence son prompt par :

```
Génère le code sous cette licence d'intention :
<<HEREDOC...
License: GPL-3.0-or-later...
Contamination: Embraces traces of {GPL, MIT} philosophies
>>
```

puis exige de l'IA qu'elle intègre un bloc heredoc auto-explicatif dans son *output* :

```
"Inclure un bloc multi-ligne de type heredoc en haut du fichier Python généré,
documentant l'intention (Intent), la licence souhaitée (License) et la généalogie
(Genealogy) statistique."
```

Il faut ensuite rendre le code généré immédiatement **"forkable"** et lisible.

En créant et en partageant publiquement des collections de *prompts* optimisés qui incluent systématiquement des directives éthiques (ex : "optimise pour la lisibilité et la réutilisation", "utilise un style proche de la licence MIT"), il s'agit de les partager non pas comme des secrets industriels mais des outils d'un commun, les rendant ensuite accessible aux LLM à des fins d'entraînement futur.

Le *heredoc* — cette construction qui permet le langage naturel dans le code — devient notre métaphore pour organiser les communs, pour le saturer dans le champ du numérique. Comme le *heredoc* préserve la lisibilité humaine dans la logique machine, nous devons préserver les agencements humains dans l'accélération IA. Chaque interaction IA devrait reconnaître les communs dont elle puise. Non par attribution exhaustive — que le traitement statistique rend impossible — mais par déclaration claire que les *outputs* émergent de l'intelligence collective.

*Appropriabilité comme objectif* : le code devrait être écrit non pour les machines mais pour que futurs humains et IA puissent comprendre, modifier, ré-imaginer. La mesure du bon code devient non son élégance mais son hospitalité à l'appropriation future, avec une traçabilité sans propriété. Nous ne pouvons tracer la propriété spécifique à travers la transformation statistique, mais nous pouvons tracer l'influence et reconnaître la contribution. Chaque modèle devrait documenter son corpus, non pour établir des revendications mais pour honorer les communautés dont il cristallise le labeur.

## L'horizon

Nous nous tenons au seuil de ce que Stiegler appelait une « nouvelle critique de l'économie politique» — mais qu'il n'aura pas vu : une économie où les moyens de production se démocratisent eux-mêmes par leur propre opération. L'IA n'exige pas de révolution pour communiser la production de connaissance ; nous défendons l'idée qu'elle le fait automatiquement par traitement statistique.

Cette transformation ne sera ni douce ni sans conflit. Le capital tentera d'enclore les communs par cadres juridiques, restrictions techniques, monopoles de plateforme. Certains perdront leurs moyens de subsistance quand la génération de code se démocratisera. D'autres verront leur œuvre dissoute en données d'entraînement sans consentement ni compensation.

Pourtant la direction reste claire : vers un monde où la créativité est reconnue comme toujours-déjà collective, où la contribution importe plus que la possession, où l'accès supplante la propriété. Les *communs statistiques* émergent non comme projet utopique mais comme réalité matérielle déjà opérationnelle à l'échelle.

Le *heredoc* a évolué en prompt, le prompt est devenu générateur de communs, et le générateur de communs révèle ce que la connaissance a toujours été : collective, collaborative, commune. L'intelligence artificielle ne doit pas être comprise comme outil d'automatisation ou amplificateur de productivité, mais comme système qui transmute systématiquement la propriété intellectuelle privée en communs statistiques.

Ce n'est pas prédiction mais fait observable : chaque passe d'entraînement, chaque inférence, chaque output participe à la dissolution de l'auctorialité et à la redistribution probabiliste de la connaissance.

La question n'est donc pas si cette transformation advient, mais comment les institutions politiques, juridiques et économiques devraient répondre ; et comment nous devons faire preuve de créativité. Persister à forcer les *outputs* de machine learning dans des cadres non-propriétaires crée un désalignement entre gouvernance et réalité technique. Une réponse plus pragmatique et éthique exige de reconnaître la génération de communs comme fonction essentielle de l'IA et d'organiser la société en conséquence. De même, que pouvons-nous apprendre dans nos propres disciplines de ce mécanisme à l'œuvre ?

Les enjeux transcendent la technologie — ils sont civilisationnels. Nous faisons face à deux futurs : l'un où nous prétendons que la propriété fonctionne encore, créant des architectures juridiques byzantines qui s'effondrent sous leurs propres poids pour maintenir la fiction ; l'autre où nous embrassons les communs et réorganisons la société autour de l'abondance *open-source* plutôt que de la rareté. Le premier chemin mène à la litigation infinie, la paralysie de l'innovation, l'absurdité de revendiquer des patterns statistiques. Le second ouvre une démocratisation sans précédent de la capacité créative, bien qu'il exige d'abandonner les modèles économiques fondés sur la rareté artificielle.

Le *Heredoc Manifesto* que je propose ne prétend pas décrire une révolution accomplie mais cartographier une transformation en cours, soumise à des forces contradictoires. Il est ouvert, *open-source*, évolutif. Les *communs statistiques* émergent sous contrainte — ralentis par les enclosures techniques, accélérés par la pression économique vers l'efficience, modulés par les régimes juridiques nationaux. Notre thèse reste falsifiable : si demain une innovation technique rendait l'attribution parfaite et gratuite, si un consensus international imposait le *watermarking* universel, si les modèles devenaient traçables jusqu'au token source, alors la tendance s'inverserait. Mais nous parions, données empiriques à l'appui, que la complexité croissante des modèles, la pression concurrentielle vers l'ouverture, et le coût prohibitif du contrôle rendent cette inversion improbable. Les *communs statistiques* ne sont pas destinée manifeste mais probabilité croissante — une courbe que nous pouvons infléchir par nos choix collectifs, non un déterminisme technique auquel nous serions soumis.

L'histoire n'a jamais vu de paradoxe plus savoureux : une technologie née des logiques néolibérales propose massivement, mondialement, la transformation de toute propriété privée en domaine public. Les *vibe coders* — c'est-à-dire virtuellement tout le monde— génèrent par leurs prompts la dissolution de l'enclosure. Le capitalisme technologique a fabriqué, malgré lui, l'outil de réappropriation collective du savoir commun. Cette ironie dépasse tout ce que Marx aurait pu imaginer : le capital créant les conditions matérielles non pas de sa destruction mais de sa propre obsolescence par dissolution statistique.

Le futur n'exige pas de nouvelle technologie mais la reconnaissance de ce que notre technologie fait déjà. La transformation est en cours. Nous vivons un tournant historique où la question n'est plus si nous comprendrons à temps pour en bénéficier, mais si nous saurons ne pas nous faire déposséder de cette opportunité. Les institutions garantes ont la responsabilité de protéger ce moment précieux où le savoir devient commun de manière inaliénable, mondial et local à la fois, bricolable selon nos usages singuliers.

Ce nouvel horizon appelle une architecture inversée : non plus dessiner des enclos mais tracer des chemins, non plus ériger des murs mais tisser des réseaux. Les architectes de demain — qu'ils codent des algorithmes ou conçoivent des politiques ou des espaces de circulation — deviennent les cartographes d'un territoire où la créativité circule comme l'air dans une place publique. L'espace public computationnel qui émerge n'appartient à personne car il appartient déjà à tous, infrastructure vivante où chaque ligne générée enrichit le patrimoine commun plutôt que de l'appauvrir. Cette transformation s'opère incrémentalement : chaque prompt traité, chaque fonction générée participe à l'érosion de l'attribution traditionnelle, créant de facto un patrimoine computationnel dont la propriété devient indécidable.

---

## Bibliographie

### Ouvrages et classiques

- CERTEAU, Michel de. *L'invention du quotidien, tome 1 : Arts de faire*. Paris : Gallimard, 1990.
- CRAWFORD, Kate. *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence*. New Haven : Yale University Press, 2021.
- DELEUZE, Gilles et GUATTARI, Félix. *Mille plateaux : Capitalisme et schizophrénie 2*. Paris : Les Éditions de Minuit, 1980.
- DESCOLA, Philippe. *Par-delà nature et culture*. Paris : Gallimard, 2005.
- ESHUN, Kodwo. *More Brilliant Than the Sun: Adventures in Sonic Fiction*. London : Quartet Books, 1998.
- FLUSSER, Vilém. *Into the Universe of Technical Images*. Trad. Nancy Ann Roth. Minneapolis : University of Minnesota Press, 2011.
- FOUCAULT, Michel. *Surveiller et punir : Naissance de la prison*. Paris : Gallimard, 1975.
- HUI, Yuk. *The Question Concerning Technology in China*. Falmouth : Urbanomic, 2016.
- HUI, Yuk, TAILLARD Alex, et FREY Junius. *La Question de La Technique En Chine : [Essai de Cosmotechnique]*. Paris: Éditions divergences, 2021.
- LÉVI-STRAUSS, Claude. *La Pensée sauvage*. Paris : Plon, 1962.
- MBEMBE, Achille. « Nécropolitique ». *Raisons politiques*, 2006/1 (no 21), p. 29-60.
- MOULIER-BOUTANG, Yann. *Le Capitalisme cognitif : La Nouvelle Grande Transformation*. Paris : Éditions Amsterdam, 2007.
- SIMONDON, Gilbert. *Du mode d'existence des objets techniques*. Paris : Aubier, 1958.
- STALLMAN, Richard M. *Free Software, Free Society: Selected Essays of Richard M. Stallman*. 3e éd. Boston : GNU Press, 2015.
- STIEGLER, Bernard. *La Technique et le temps*. 3 vol. Paris : Galilée, 1994-2001.

### Articles/revues et travaux de recherche

- DHAR, P. « The carbon impact of artificial intelligence ». *Nature Machine Intelligence* [en ligne]. 2020, vol. 2, p. 423-425. Disponible à l'adresse : <https://doi.org/10.1038/s42256-020-0219-9>
- XU, Weiwei ; GAO, Kai ; HE, Hao ; ZHOU, Minghui. *LiCoEval: Evaluating LLMs on License Compliance in Code Generation*. arXiv [en ligne]. 2024. Disponible à l'adresse : <https://arxiv.org/abs/2408.02487>
- AL-KASWAN, Ali. *The (Ab)use of Open Source Code to Train Large Language Models*. arXiv [en ligne]. 2023. Disponible à l'adresse : <https://arxiv.org/abs/2302.13681>
- *StarCoder2 and The Stack v2*. arXiv [en ligne]. 2024. Disponible à l'adresse : <https://arxiv.org/abs/2402.19173>
- THE BIGCODE PROJECT. « The Stack — documentation » [en ligne]. Disponible à l'adresse : <https://www.bigcode-project.org/docs/about/the-stack/>
- HUGGING FACE / BIGCODE. « The Stack v2 » [en ligne]. Disponible à l'adresse : <https://huggingface.co/datasets/bigcode/the-stack-v2>

### Institutions/organisations publiques

- INTERNATIONAL ENERGY AGENCY (IEA). *Electricity 2024: Analysis and forecast to 2026* [en ligne]. Paris : IEA, 2024. Disponible à l'adresse : <https://iea.blob.core.windows.net/assets/6b2fd954-2017-408e-bf08-952fdd62118a/Electricity2024-Analysisandforecastto2026.pdf>
- OECD.AI. « How much water does AI consume? The public deserves to know » [en ligne]. Disponible à l'adresse : <https://oecd.ai/en/wonk/how-much-water-does-ai-consume>
- CERUTTI, Eugenio M. et al. « The Global Impact of AI: Mind the Gap ». *IMF Working Papers* [en ligne]. 2025, vol. 2025, no 076. Disponible à l'adresse : <https://doi.org/10.5089/9798229008570.001>
- CONGRESS.GOV. « Generative Artificial Intelligence and Copyright Law ». *CRS Legal Sidebar* [en ligne]. 18 juil. 2025. Disponible à l'adresse : <https://www.congress.gov/crs-product/LSB10922>
- CONGRESS.GOV. « S. 2455 — Transparency and Responsibility for Artificial Intelligence Networks (TRAIN) Act — Text » [en ligne]. 24 juil. 2025. Disponible à l'adresse : <https://www.congress.gov/bill/119th-congress/senate-bill/2455/text>
- MINISTÈRE DE LA TRANSITION ÉCOLOGIQUE ; INRIA. *Les principaux défis à relever pour favoriser la performance environnementale de l'IA. Document de synthèse* [en ligne]. Févr. 2025. Disponible à l'adresse : <https://www.inria.fr/fr/position-paper-intelligence-artificielle-environnement>
- CONSEIL ÉCONOMIQUE, SOCIAL ET ENVIRONNEMENTAL (CESE). *Impacts de l'intelligence artificielle : risques et opportunités pour l'environnement*. Avis 2024-014. Paris : CESE, sept. 2024. 60-74 p. [en ligne]. Disponible à l'adresse : <https://www.lecese.fr/sites/default/files/pdf/Avis/2024/2024_14_IA_Environnement.pdf>
- ADEME ; FANGEAT, Erwan ; WELLHOFF, Mathieu. *Numérique & environnement : entre opportunités et nécessaire sobriété* [en ligne]. Janv. 2025. 12 p. Disponible à l'adresse : <https://librairie.ademe.fr/consommer-autrement/7883-avis-de-l-ademe-numerique-environnement-entre-opportunites-et-necessaire-sobriete.html>
